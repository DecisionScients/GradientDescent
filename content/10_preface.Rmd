# Introduction
Gradient descent is among *the* most important algorithms in machine learning and is the defacto standard for optimizing deep learning networks. First introduced by Louis Augustin Cauchy in 1847 [@Cauchy1847Methode], Gradient Descent has evolved into a family of optimization algorithms and is now the workhorse behind state-of-the-art deep learning frameworks such as Keras [@chollet2015keras], TensorFlow [@tensorflow2015-whitepaper], Caffe [@jia2014caffe], Lasagne [@lasagne], and PyTorch [@paszke2017automatic], to name a few. 

During this series, we will:    

* Develop our intuition and command of gradient descent and the theory behind it.           
* Build a custom, extensible, and modular Gradient Descent lab in Python from the ground up. 
* Analyze gradient descent performance vis-a-vis its hyperparameters and variants. 

By the close of this series you will have a performant, reliable, and extensible implementation that will support:    
* Batch, minibatch, and stochastic gradient descent,    
* Linear regression, logistic regression and softmax (multinomial) regression,    
* Lasso, ridge, and elastic net regularization techniques

## Learning Objectives
The overall goal is to understand gradient descent, its applications, and variants so that we can:  

* Meet a wider range of real-world optimization challenges, independent of specific frameworks   
* Customize and apply existing frameworks more effectively     
* Create custom implementations of algorithms for learning, specialization, or experimentation purposes

At the completion of this series, we should be able to:   

* Contrast batch, minibatch, and stochastic gradient descent performance and behavior,     
* Understand the mathematics and theory behind gradient descent,          
* Assess the behavior and performance of gradient descent and its variants,    
* Intuit and reason about hyperparameters and their effects on algorithm performance,    
* Apply regularization techniques to linear regression, logistic regression, and softmax regression problems,  
* Bolster your ability to develop modular, performant, and reliable machine learning algorithms

## Why implement machine learning algorithms from scratch?

Why build machine learning algorithms from scratch when there is no dearth of highly optimized and tested open-source packages and frameworks that I can use out-of-the-box without a deep understanding of optimization methods, gradients and loss functions. Is the return worth the investment in time? 

![](figures/feynman.jpg)

`r kfigr::figr(label = "feynman", prefix = TRUE, link = TRUE, type="Figure")` (Richard Feynman, February 15, 1988) &copy; California Institute of Technology 

Well, there are several reasons why building an algorithm from scratch may make sense:

* **Learning motivation.** The relative ease with which existing libraries can be deployed can create the illusion of competence when the underlying data science is not well understood. Paraphrasing theoretical physicist, Richard Feynman, we don't understand that which we cannot create. Building machine learning algorithms deepens our intuition into the algorithm's behavior, reveals the effects of their hyperparameters and reinforces the underlying mathematical principles. Consequently we:    

  + gain a deeper appreciation for existing, battle-tested, and optimized implementations,   
  + adapt to new frameworks and APIs with greater agility,   
  + explain, with greater credibility, how and why an algorithm performs to colleagues and stakeholders    
  
* **Specialization** We may need to create a custom implementation because we are not satisfied with the 'features' of existing implementations. Suppose they don't support KL Divergence loss or cyclical learning rate schedules with stochastic Gradient Descent. Perhaps you've discovered a research paper describing an innovative approach to early stopping and you want to run computational complexity experiments. By implementing algorithms, we are able to fine-tune the functionality to suit specific project requirements. This can improve predictive or computational performance.

* **Experimentation** Once you've implemented a machine learning algorithm, you can perform studies to evaluate the predictive and computational performance of various parameters and new functionality. This can lead to new insights and discoveries for future implementations that you can contribute and promote.

## Organization

1. Context and Concepts    
  * Introduction to Optimization      
  * Gradient-Based Optimization
  
2. Linear Regression    
  * Gradient descent for Linear Regression            
  * Design and Implementation       
  * Performance and Behavior Analysis
  
3. Logistic Regression    
  * Gradient descent for Logistic Regression            
  * Design and Implementation       
  * Performance and Behavior Analysis  
 
4. Softmax Regression    
  * Gradient descent for Softmax Regression            
  * Design and Implementation       
  * Performance and Behavior Analysis
  
## Cases     
Throughout this series, we will be using three of the most widely used datasets in machine learning.    
* Part I - Linear Regression: [Boston Housing Dataset (Regression)](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html)     
* Part II - Logistic Regression: [Wisconson Breast Cancer Dataset (Binary Classification)](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html)    
* Part III - Softmax Regression: [The Iris Dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html)

## Prerequisites
This series assumes the reader has a basic to intermediate facility with the Python programming language. You should also be familiar with the [Numpy](http://www.numpy.org/) and [Pandas](https://pandas.pydata.org/) libraries.  Exposure to the [scikit-learn](https://scikit-learn.org/stable/) library would be advantageous, but is not required. Some background in multivariate calculus is also useful but not required.