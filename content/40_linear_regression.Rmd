<script type="text/x-mathjax-config">
MathJax.Hub.Config({
TeX: {
equationNumbers: {
autoNumber: "all",
formatNumber: function (n) {return ''+n}
}
}});</script>

# Linear Regression with Gradient Descent from Scratch
## Introduction
This is section __ of a 6-part series on Gradient Descent. In prior sections, we examined the calculus behind Gradient Descent and explored the relative merits of the Gradient Descent variants: Batch Gradient Descent, Stochastic Gradient Descent and Minibatch Gradient Descent.

During this section, we will design, build, and evaluate a Gradient Decent optimizer for Linear Regression.  Concretely, we will:  

* define the hypothesis space, objective function and its derivative for optimization      
* implement the three variants, Batch, Stochastic, and Minibatch Gradient Descent     
* apply the algorithm to three (3) real-world Linear Regression problems            
* analyze the effect of hyperparameters on algorithm behavior, accuracy, and speed  
* evaluate performance vis-a-vis available performance benchmarks for each problem   

The overall goal is to understand the basics of Gradient Descent and its variants so that we can:  

* Meet a wider range of real-world optimization challenges, independent of specific frameworks   
* Customize and apply existing frameworks more effectively     
* Create custom implementations of algorithms for learning, specialization, or experimentation purposes   

At the end of this post, you will have a reasonably performant and well-written Gradient Descent optimizer, a custom laboratory, that you can explore and enhance with new innovations from the research community.    

### Why implement machine learning algorithms from scratch?

Why build machine learning algorithms from scratch when there is no dearth of highly optimized and tested open-source packages and frameworks that I can use out-of-the-box without a deep understanding of optimization methods, gradients and loss functions. Is the return worth the investment in time? 

![](figures/feynman.jpg)

`r kfigr::figr(label = "feynman", prefix = TRUE, link = TRUE, type="Figure")` (Richard Feynman, February 15, 1988) &copy; California Institute of Technology 

Well, there are several reasons why building an algorithm from scratch may make sense:

* **Learning motivation.** The relative ease with which existing libraries can be deployed can create the illusion of competence when the underlying data science is not well understood. Paraphrasing theoretical physicist, Richard Feynman, we don't understand that which we cannot create. Building machine learning algorithms deepens our intuition into the algorithm's behavior, reveals the effects of their hyperparameters and reinforces the underlying mathematical principles. Consequently we:    

  + gain a deeper appreciation for existing, battle-tested, and optimized implementations,   
  + adapt to new frameworks and APIs with greater agility,   
  + explain, with greater credibility, how and why an algorithm performs to colleagues and stakeholders    
  
* **Specialization** We may need to create a custom implementation because we are not satisfied with the 'features' of existing implementations. Suppose they don't support KL Divergence loss or cyclical learning rate schedules with stochastic Gradient Descent. Perhaps you've discovered a research paper describing an innovative approach to early stopping and you want to run computational complexity experiments. By implementing algorithms, we are able to fine-tune the functionality to suit specific project requirements. This can improve predictive or computational performance.

* **Experimentation** Once you've implemented a machine learning algorithm, you can perform studies to evaluate the predictive and computational performance of various parameters and new functionality. This can lead to new insights and discoveries for future implementations that you can contribute and promote.

### Case Studies
We will evaluate our optimizer on three datasets, each presenting a different optimization challenge in terms of number of features and overall database size. 

#### Boston Housing Dataset
The Boston Housing Dataset contains information collected by the U.S Census Service concerning housing in the area of Boston Mass. The dataset was original published by by Harrison, D. and Rubinfeld, D.L. [@Harrison] in their 1978 study the use of housing market data to ascertain willingness ot pay for clean air. The dataset has been used extensively throughout the literature to benchmark algorithms. The dataset, with 506 observations and 13 real-valued features, was obtained from [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html). Our task is to predict median housing prices based upon features such as per capita crima rate, average number of rooms per dwelling, pupil-teacher ratio, age of the homes and others. It is currently maintained by Carnegie Mellon University, where the [codebook](http://lib.stat.cmu.edu/datasets/boston) and data may be obtained.

The evaluation metric for this case will be the root mean squared error (RMSE):
$$\text{RMSE}(y)=\sqrt{\displaystyle\sum_{i=1}^m(y_i-\hat{y})^2}$$
The performance benchmark is to be within the top 50 scores for the 2016 [Kaggle Boston Housing competition](https://www.kaggle.com/c/boston-housing/overview/evaluation). As of the time of this writing, the 50th score on the leaderboard was $\text{RMSE}=4.11517$.

#### Facebook Comment Volume Prediction
The Facebook Comment Volume Prediction Dataset was obtained by Kamaljot Singh to study and model social network user activity patterns [@facebook]. The dataset contains 40949 observations of Facebook Pages, with  54 features such as:

* Page popularity likes   
* Page checkins     
* Number of comments in last 24 hours, relative to a base date/time          
* Number of comments in first 24 hours after publication, but before base date/time
* Post share count     
* Post published weekday    

The task is to predict the number of comments in H hours (H is a given feature). The dataset was obtained from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Facebook+Comment+Volume+Dataset)

We will be using the results of Singh's 2015 report[@facebook] as a performance benchmark. He used three metrics to evaluate performance of several models.

**Hits@10**
The observations which were predicted to have the top 10 number of comments were counted among the observations that were the actual top 10 observations in terms of comments. This was the measurement of accuracy.

Singh was able to achieve HITS@10 scores from $4.6\pm0.800$ to $6.4\pm1.200$.

**AUC@10**
The Area Under the Receiver-Operator Curve (AUC), he considered the true top 10 blog pages receiving the hightest number of comments as positive. The AUC was computed as follows:
$$\text{AUC}=\frac{T_p}{T_p+F_p}$$
where:      
* $T_p$ was the number of predicted top 10 blog pages that were also in the true top 10 blog pages     
* $F_p$ was the number of predicted top 10 blog pages that were not in the true top 10 blog pages     

For Singh's experiments, AUC@10 scores ranged from $0.704\pm0.168$ to $0.878\pm0.085$.

**Computation Time**
Training computation time in seconds was obtained for each of Singh's models. Times ranged from 6-145 seconds. 

#### Million Song Dataset (MSD) Year Dataset
The Million Song Dataset (MSD) [@Bertin-Mahieux2011] is a freely-available collection of audio features and metadata for a million contemporary popular music tracks. This is a subset of the MSD and contains audio features of songs with the year of the song. The purpose of this set is to predict the release year of a song from its audio features. Songs are mostly western, commercial tracks ranging from 1922 to 2011, with a peak in the year 2000s. There are 515,345 observations and 90 features. Among the 90 features, 12 are timbre averages and 78 are timbre covariance.

The performance metric that we will use is mean squared error (MSE), defined as:
$$\text{MSE}=\frac{1}{m}\displaystyle\sum_{i=1}^m(y_i-\hat{y}_i)^2$$

## Linear Regression Optimization Model
Linear Regression is routinely used in business settings to evaluate trends, make estimates or forecasts, predict faults, and assess risk. It is used to generate insights into customer behvavior, reveal factors influencing profitability, and analyze marketing, pricing, sales, and promotional effectiveness. Predictive analytics, operational efficiency, and decision support, are among the many applications that make Linear Regression the most widely-used statistical technique, by far.

Linear regression is the study of linear, additive relationships between variables. Given:

* a set of $m$ training examples with independent variables $X=\{x^{(1)},x^{(2)},\dots,x^{(m)}\}$, and dependent variable $Y=\{y^{(1)},y^{(2)},\dots,y^{(m)}\}$, 
* where each example contains a feature vector $x\in \mathbb{R}^N$ real-valued attributes $x^{(i)T}=\{x^{(i)}_1,x^{(i)}_2,\dots, x^{(i)}_n \}$.   

our task is to find a hypothesis function $h(x)$ that best approximates the true target function $f(x)$ such that $\forall x, h(x) \approx f(x)$.

### Linear Regression Assumptions
We suppose that the true data generating model is:
$$Y=\theta_0+\displaystyle\sum_{i=1}^n\theta_ix_i+\epsilon$$
where:    

* $\theta_0$ is the bias term     
* $\theta_i$ is the $i$th coefficient, or parameter    
* $x_i$ is the $i$th variable in $x$    
* $\epsilon$ is the error term         
* $n$ is the number of variables or features in $x$    

Four principal assumptions will shape the hypothesis space, the space of candidate functions $h(x)$.

**1. Linearity**:The central property of this function is that the expected value of the dependent variable $Y$ is a linear function of each of the independent variables, holding the others constant.      
**2. Additivity**: The effects of the different independent variables on the expected value of the dependent variable are additive.     
**3. Errors are I.I.D.**: Errors $\epsilon$ are independently and identically normally distributed such that $\epsilon \sim \mathcal{N}(0,\sigma^2)$      
**4. Homoscedasticity**: Errors $\epsilon$ have a constant variance vis-a-vis predictions and independent variables.    

### Linear Regression Hypothesis Space
Given the assumptions above, our hypothesis space is the set of linear functions of the form:
$$h_\theta(x)=\theta_0+\displaystyle\sum_{i=1}^n\theta_ix_i$$
To simplify our notation, we can add an intercept term $x_0=1$ to our feature vector $x$. Now, we can treat $\theta_0$ as just another parameter to learn. This simplifies our notation because we can now represent our hypothesis in the *vectorized* form:
$$h_\theta(x)=\displaystyle\sum_{i=0}^n\theta_ix_i=\theta^Tx$$
The variables $X$ can come from different sources:  

* quantitative inputs,     
* transformations of quantitative inputs, such as log, square-root or square,      
* basis expansions, such as $X_2=X_1^2, X_3=X_1^3$ leading to a polynomial representation,     
* numeric or "dummy" coding of levels of quantitative inputs,     
* interactions between variables, such as $X_3=X_1\times X_2$

Regardless of the source, the hypothesis space is linear in the parameters  [@hastie01statisticallearning]. We assume a linear relationship between the independent and dependent variables because:    

* linear relationships are the simplest *non-trivial* relationships that one can imagine,    
* linear relationships are the easiest to work with,    
* the 'true' relationships between our variables are often at least *approximately* linear over the range of values of interest,       
* non-linear relationships can be linearized by transforming the variables    

Each parameter $\theta_1, \theta_2, \dots, \theta_n$ is a real number, and is associated with one of the input features $x_1, x_2, \dots, x_n$. The parameter $\theta_j$ represents how important that input $x_j$ is to the hypothesis or prediction. The additional constant $\theta_0$, the bias term, is the prediction that the model would make if all the $x$'s were zero.

### Linear Regression Cost Function
Having defined our hypothesis space, our next step is to specify a *cost* function to measure a penalty when predicting $\hat{y}$ when the true value of the dependent variable is $y$. The overall cost is given by:
$$J(\theta)=\frac{1}{2m}\displaystyle\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2$$
This **quadratic** function, also known as the least squares or Mean Squared Error (MSE) cost function, is convex with a single global minimum, well suited for Gradient Descent optimization. Note that this is not actually the mean squared error formula. We've added the $\frac{1}{2}$ factor as a mathematical convenience when computing the gradient. Nonetheless, the $\frac{1}{2}$ factor doesn't affect the optimization problem. The parameters $\theta$ that minimize $MSE$ are the same as those that minimize $\frac{1}{2}MSE$.  

There are several other cost functions used in regression settings, including Mean Squared Logarithmic Error Loss and Mean Absolute Error Loss, but the quadratic, Mean Squared Error (MSE) cost function is essentially the default for regression problems.  Why? It has to do with **maximum likelihood estimation (MLE)**. 

#### Maximum Likelihood Estimation (MLE)
MLE is an approach for estimating the parameter or parameters $\theta$ of a model. Given a dependent variable $y=y_1, y_2,\dots, y_m$, a feature matrix $X_1, X_2,\dots, X_3$ and parameters $\theta$ and $\sigma^2$, MLE measures the propensity that the data was generated from the model. The parameter estimate, obtained by maximizing a **likelihood function** is called the maximum likelihood estimate $\hat{\theta}_{MLE}$ 

##### Likelihood Function
Recall that our statistical model is defined as follows:    

* The distribution of $X$ is arbitrary and perhaps non-random.     
* $y=\theta_0+\displaystyle\sum_{i=1}^n\theta_ix_i+\epsilon$    
* $\hat{y}=\theta_0+\displaystyle\sum_{i=1}^n\theta_ix_i$    
* Adding the bias term to the feature matrix $X$, $\hat{y}=\theta^Tx$
* $\epsilon \sim mathcal{N}(0,\sigma^2)$, and is independent of X.      
* $\epsilon$ is independent across observations.    

Consequently $Y$ is independent across observations and $Y$ is normally distributed around $\hat{Y}$ as such, $y\sim \mathcal{N}(\hat{y},\sigma^2)$. Because of these strong assumptions, we can express the conditional probability of observing the output $y$, given $x$ and the model parameters $\theta$ and $\sigma^2$ as the Guassian Probability Density Function (PDF):  
$$p(y|x;\theta,\sigma^2)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(y-\mu)^2}{2\sigma^2}}$$
Since we don't know the true population parameters, we substitute:    

* $\mu$ with $\hat{y}=\theta^Tx$   
* $\sigma^2$ with $s^2$, the sample variance

This yields the probability density function of $Y$ for each $x$ as:
$$p(y|x;\theta,s^2)=\frac{1}{\sqrt{2\pi s^2}}e^{-\frac{(y-\theta^Tx)^2}{2s^2}}$$
The Guassian probability density functions with various parameterizations are plotted below. The red distribution is the standard Gaussian $P(x) \sim \mathcal{N}(0,1)$.
![](./figures/pdf.png)

`r kfigr::figr(label = "sigmoid", prefix = TRUE, link = TRUE, type="Figure")`: A selection of Gaussian Probability Density Functions (PDFs) **Source**: Wikimedia Commons

The **likelihood function** for the training set is the product of the individual probability densities.
$$\mathcal{L}(\theta,s^2)=\prod_{i=1}^mp(y^{(i)}|x^{(i)};\theta,s^2)=\prod_{i=1}^m\frac{1}{\sqrt{2\pi s^2}}e^{-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2s^2}}\label{}$$
Probability densities are relatively small numbers and multiplying them may lead to arithmetic underflow. So its often easier to work with the natural logarithm of the likelihood function, which we will call the **log-likelihood**. Taking the log of equation ($\ref{likelihood}$) and applying the product rule, the log-likelihood becomes:

$$
\begin{align}
\text{log }(\mathcal{L}(\theta, s^2))&=\text{log }\prod_{i=1}^mp(y^{(i)}|x^{(i)};\theta,s^2)\\
&=\displaystyle\sum_{i=1}^m\text{log }p(y^{(i)}|x^{(i)};\theta,s^2)\\
&=-\frac{m}{2}\text{log }2\pi-m\text{ log }s-\frac{1}{2s^2}\displaystyle\sum_{i=1}^m(y^{(i)}-(\theta^Tx^{(i)}))^2
\end{align}
$$
Since the logarithm is a monotonically increasing function, that is to say that if $x_1>x_2$, then $\text{log}(x_1)>\text{log}(x_2)$, the value that maximizes the likelihood is also the value that maximizes the log-likelihood function. 

To find the value of $\theta$ that maximizes the log-likelihood function, we find its critical point, the point at which the function's partial derivative with respect to parameters $\theta$ is zero:
$$\frac{\partial\mathcal{L}(\theta)}{\partial \theta}=0$$
We can remove all the terms that do not contain our parameter $\theta$ as they do not have any effect on the optimization problem. Hence, our objective function can be reduced to:
$$
\begin{align}
\text{log}(\mathcal{L}(\theta, s^2))&=-\displaystyle\sum_{i=1}^m{(y-\theta^Tx^{(i)})^2}
\end{align}
\label{ref:ll}
$$
Therefore, the value $\theta$ that maximizes equation ($\ref{ll}$) is the maximum likelihood estimate $\hat{\theta}_{MSE}$:
$$\hat{\theta}_{MSE}=\text{arg max}\bigg[-\displaystyle\sum_{i=1}^m(y^{(i)}-\hat{y}^{(i)})^2\bigg]\label{ref:mse}$$
Maximizing a function is the same as minimizing its negative, so we can rewrite (${\ref{mse}}$) as:
$$\hat{\theta}_{MSE}=\text{arg min}\displaystyle\sum_{i=1}^m(y^{(i)}-\hat{y}^{(i)})^2$$
Taking the average across all observations, we get:
$$\hat{\theta}_{MSE}=\text{arg min}\frac{1}{m}\displaystyle\sum_{i=1}^m(y^{(i)}-\hat{y}^{(i)})^2$$
Viola! Aside from the 2 in the denominator of the cost function, this is precisely the quadratic (MSE) cost function. Maximizing the likelihood function is the same as minimizing the quadratic (MSE) cost function. This the *real* reason that the quadratic MSE cost function is so widely used in regression contexts.

### Gradient of the Linear Regression Cost Function
We've justified our cost function in terms of maximum likelihood estimation. Now, we compute the gradient of the cost function, with respect to parameters $\theta$, in order to determine the direction that decreases the cost function the most. Recall, the gradient, $\nabla J(\theta)$, is a vector-valued function, in which the vector contains the partial derivatives of the cost function with respect to each of the parameters $\theta$. We derive $\nabla J(\theta)$ as follows:

$$
\begin{equation}
\begin{split}
\frac{\partial}{\partial\theta_j}J(\theta)&=\frac{\partial}{\partial \theta_j}\bigg(\frac{1}{2m}\displaystyle\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2\bigg)\\
\frac{\partial}{\partial\theta_j}J(\theta)&=\frac{1}{2m}\displaystyle\sum_{i=1}^m\frac{\partial}{\partial \theta_j}(h_\theta(x^{(i)})-y^{(i)})^2\hspace{50mm}\text{derivative of sum is sum of derivative}\\
\frac{\partial}{\partial\theta_j}&=\frac{1}{2m}\displaystyle\sum_{i=1}^m2(h_\theta(x^{(i)})-y^{(i)})\frac{\partial}{\partial \theta_j}(h_\theta(x^{(i)})-y^{(i)})\hspace{22mm}\text{power+chain rule}\\
\frac{\partial}{\partial\theta_j}&=\frac{1}{2m}\displaystyle\sum_{i=1}^m2(h_\theta(x^{(i)})-y^{(i)})\frac{\partial}{\partial \theta_j}(\displaystyle\sum_{j=0}^n\theta_j^Tx_j^{(i)}-y^{(i)})\hspace{17mm}\text{substituting}\space \theta^Tx\\
\nabla J_\theta&=\frac{\partial}{\partial\theta_j}=\frac{1}{m}\displaystyle\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}\space(\text{for every j})\hspace{19mm}\text{power rule+algebraic simplification}
\end{split}
\end{equation}
$$
Again, this formulation assumes that we have included a bias term $x_0=1$ 


### Linear Regression with Gradient Descent Parameter Update Rule
Now that we have our gradient, we can iteratively apply the least mean squares (LMS) update rule.
$$
\theta_j := \theta_j-\alpha\nabla J_\theta
$$
The LMS update rule has several properties that seem natural. For instance, we update the parameters $\theta$ by *subtracting* the scaled gradient in order to move in the direction of steepest *descent*. In addition, the magnitude of the update is proportional to the **error** term $h_\theta(x^{(i)})-y^{(i)}$. If we encounter large errors in our training batch, the magnitude of the gradient is higher resulting in a larger change to the parameters $\theta$. On the other hand, if our predictions are close to the true values $y$, our error term is smaller, resulting in smaller changes to the parameters. Lastly, Gradient Descent can be susceptible to local minima; however, the error manifold for the linear regression cost function is convex and as such, has only a single global minimum.


### Linear Regression Model Summary
To summarize, the linear regression model representation is:
$$
\begin{equation}
\begin{split}
\text{Hypothesis Function}\hspace{30mm}&h_\theta(x)=\theta^Tx\\
\text{Cost Function}\hspace{30mm}&J(\theta)=\frac{1}{2m}\displaystyle\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2\\
\text{Gradient}\hspace{30mm}&\nabla{J\theta}=\frac{1}{m}\displaystyle\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}\space(\text{for every j})\\
\text{Parameter Update Rule}\hspace{30mm}&\theta_j := \theta_j-\alpha\nabla J_\theta
\end{split}
\end{equation}
$$

## Design 
Before we dive into code, let's align ourselves on a few design principlesthat will support current requirements as well as future enhancements. 

### Design Principles     
* **Separation of Concerns**. The model is designed to be a cooperation between cohesive and loosely coupled, and fully configurable modules that can be easily plugged together. Metrics, cost functions and their gradients, and enhancements such as learning rate schedules, early stopping regimes, and optimizers should be separate modules connected by an abstraction layer. 

* **SOLID Foundation**. Adhere to proven principles of object-oriented programming. Ensure that a class does *one* thing, hopefully well. Classes can be extended without affecting existing functionality. Separate interfaces, such as Linear Regression, Ridge Regression, Logistic Regression from estimation. We should be able to replace an object with its subtype without affecting the code. Implementation depends upon abstractions, not the other way around.

* **Extensibility**. Build for extensibility. We should be able to quickly design, integrate, and evaluate enhancements and innovations from the research community, and with minimal impact to existing structures. Code should be easy to read and modify. We must be able to support new requirements without causing new errors.   

* **Interoperability**. The estimator should be scikit-learn compatible, and adhere to the scikit-larn interface and standards. The primary motivation is that we would like to use it together with the model evaluation and selection tolls such as [GridSearchCV](https://scikit-learn.org/dev/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) and [Pipeline](https://scikit-learn.org/dev/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline). As such, we will be inheriting from [BaseEstimator](https://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html), [RegressorMixin](https://scikit-learn.org/stable/modules/generated/sklearn.base.RegressorMixin.html), and ultimately [ClassifierMixin](https://scikit-learn.org/stable/modules/generated/sklearn.base.ClassifierMixin.html).

* **Borrow Good Ideas**. Leverage best practices, novel solutions to problems, creative approaches and ideas.  Good artists copy, great artists steal!


### System Architecture
With that, let's layout the key classes, entitities and their relationships.   
![](./figures/ML Studio 1.0.png)

`r kfigr::figr(label = "architecture", prefix = TRUE, link = TRUE, type="Figure")`: 
The class diagram depicts 17 classes and 3 functions, organized into the following 5 groups.    

* Core classes (Orange) - Includes the Estimator base class and the Regression and Linear Regression subclasses.  
* Cost (Blue) - Classes responsible for computing the cost and gradients of the cost.     
* Metrics (Green) - Classes responsible for computing various metrics.       
* DataManager (Yellow) - Classes and functions used for data management and manipulation.    
* Callbacks (White) - CallbackList and Callback base class as well as the History and Progress subclasses.    

We'll implement the core classes as well as the Cost, Metrics and Callback classes together. You may obtain the DataManager from [ML Studio](https://github.com/decisionscients/ml-studio/blob/master/ml_studio/utils/data_manager.py).

## Build
### Metrics 
We'll begin with the metrics classes. We will be supporting:     
* Mean Squared Error $\text{MSE}=\frac{1}{m}\displaystyle\sum_{i=1}^m(y_i-\hat{y_i})^2$     
* Mean Absolute Error $\text{MAE}=\frac{1}{m}\displaystyle\sum_{i=1}^m(y_i-\hat{y_i})$          
* Root Mean Squared Error $\text{RMSE}=\sqrt{\frac{1}{m}\displaystyle\sum_{i=1}^m(y_i-\hat{y_i})^2}$  
* R2 The Coefficient of Determination $1-\frac{\displaystyle\sum_i(y_i-f_i)^2}{\displaystyle\sum_i(y_i-\overline{y})^2}$

We'd like to separate regression metrics from classification metrics for validation purposes, so let's create some Abstract Base Classes to separate these computations. 

```{python metrics_base, eval=F, echo=T, code=readLines('../gradient_descent/v1_linear_regression/metrics.py')[4:34]}
```

#### Mean Squared Error
```{python metrics_mse, eval=F, echo=T, code=readLines('../gradient_descent/v1_linear_regression/metrics.py')[56:72]}
```
#### Mean Absolute Error
```{python metrics_mae, eval=F, echo=T, code=readLines('../gradient_descent/v1_linear_regression/metrics.py')[38:54]}
```
#### Root Mean Squared Error
```{python metrics_rmse, eval=F, echo=T, code=readLines('../gradient_descent/v1_linear_regression/metrics.py')[73:89]}
```
The Metrics class attributes have been created to facilitate an early stopping capability that evaluates performance vis-a-vis these metrics each epoch. 

#### Coefficient of Determination (R2)   
```{python metrics_rmse, eval=F, echo=T, code=readLines('../gradient_descent/v1_linear_regression/metrics.py')[90:146]}
```
Lastly, we'll introduce a factory class to return the requested RegressionMetric object.
```{python metrics_factory, eval=F, echo=T, code=readLines('../gradient_descent/v1_linear_regression/metrics.py')[148:157]}
```

Before we move on, let's make sure the Metrics classes work as intended. Let's mock up some test data.
```{python metrics_test_data, eval=F, echo=T, code=readLines('../conftest.py')[4:16]}
```
Ok, let's bang up a pytest test class. 

```{python metrics_test_data, eval=F, echo=T, code=readLines('../tests/test_metrics.py')[5:34]}
```

```{python metrics_test_results, eval=T, echo=T}
pytest.main(["-m", "metrics"])
```

Ok, solid on the metrics.  Let's move on to the Cost class.

### Cost
Again, we would like to separate regression and classification cost functions, so let's lay down some abstract base classes.  
```{python metrics_base, eval=F, echo=T, code=readLines('../gradient_descent/v1_linear_regression/cost.py')[4:31]}
```
Recall, our cost function and its gradient are defined as:     
$$
\begin{equation}
\begin{split}
\text{Cost Function}\hspace{30mm}&J(\theta)=\frac{1}{2m}\displaystyle\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2\\
\text{Gradient}\hspace{30mm}&\nabla{J\theta}=\frac{1}{m}\displaystyle\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}\space(\text{for every j})\\
\end{split}
\end{equation}
$$

We'll expose a __call__ method for the cost computation and a method to compute the gradient.
```{python metrics_base, eval=F, echo=T, code=readLines('../gradient_descent/v1_linear_regression/cost.py')[59:78]}
```
Next, we'll expose a factory to return the requested Cost class object. This will give us the flexibility to introduce other cost functions such as Mean Absolute Error and Huber Loss down the road.
```{python metrics_base, eval=F, echo=T, code=readLines('../gradient_descent/v1_linear_regression/cost.py')[79:86]}
```
To test the Cost class, we've fabricated some test data and expected results in excel. Let's load it in.
```{python cost_test_data, eval=F, echo=T, code=readLines('../conftest.py')[17:23]}
```
Let's run the test class.
```{python cost_test, eval=F, echo=T, code=readLines('../tests/test_cost.py')[5:25]}
```
OK, what do we have?
```{python cost_test_results, eval=T, echo=T}
pytest.main(["-m", "cost"])
```
Boom! Cost done!

### Callbacks
Borrowing ideas is a cardinal design principle, not only for software engineering, but for life in general. Full disclosure. I totally swiped the callback concept from the people at Keras. It’s an elegant approach to organizing and managing functions that are applied at various stages of the training process. Callbacks can be used to view internal states, compute and report statistics, or manipulate the model itself. 

Two classes support the callback functionality.

* Callback - The abstract base class from which all callbacks inherit     
* CallbackList - An aggregation class that invokes the methods of the callbacks at various points during the training process.

#### Callback Class
```{python callbacks, eval=F, echo=T, code=readLines('../gradient_descent/v1_linear_regression/callbacks.py')[10:116]}
```

Each callback maintains a copy of the parameters for the model as well as a reference to the model itself. The methods of the form 'on_xxxx_begin' and 'on_xxxx_end'are executed at the designated points in the training process.

#### CallbackList Class
```{python callbacklist, eval=F, echo=T, code=readLines('../gradient_descent/v1_linear_regression/callbacks.py')[120:259]}
```


### Monitor Training Progress
We will be using the concept of the callback to track history and to report progress during the training process.

#### History
The History class tracks data and statistics obtained during the training process. attributes include:       

* total_epochs
* total_batches
* start_time
* end_time
* duration
* epoch log
* batch log

The epoch log contains statistics obtained each epoch, such as training cost, training scores, and if a early stopping is invoked, validation set statistics. The batch log includes the batch data, batch size, training cost and parameter data gathered each batch.

```{python history, eval=F, echo=T, code=readLines('../gradient_descent/v1_linear_regression/monitor.py')[14:75]}
```

There is a Progress class and summary function that report statistics during training and end of training respectively. They can be obtained at [ML Studio](https://github.com/decisionscients/ml-studio/blob/master/ml_studio/supervised_learning/training/monitor.py).

### Estimator
The Estimator is a base class that performs the Batch, Stochastic, and Minibatch Gradient Descent optimization for regression, and ultimately classification problems. The algorithm performed is determined by the batch size, where:    

```{python algorithm_batch_size, eval=T, echo=F}
df = pd.read_csv("./notes/algorithm_batch_size.csv")
```

```{r algorithm_batch_size_table, eval=T, echo=F, results='asis'}
kable(py$df) %>%
  kable_styling(bootstrap_options=c("striped", "hover", "condensed"), full_width=T)
```
The parameters are:     

* **learning_rate**: The learning rate applied to the gradient for parameter updates. Defaults to 0.01.     
* **batch_size**: The number of observations per batch. Defaults to None, where batch_size = total sample size    
* **theta_init**: Initial values for the parameters. Defaults to None.    
* **epochs**: The total number of epochs to execute.    
* **cost**: A string indicating the cost function. Defaults to 'quadratic' for regression problems.    
* **metric**: The metric used to evaluate performance. Defaults to 'mean_squared_error'. Also supports 'mean_absolute_error' and 'root_mean_squared_error'.      
* **early_stop**: Boolean or an EarlyStop instance. Defaults to False      
* **val_size**: The proportion of hte training set to allocate to the validation set. Defaults to 0.3     
* **precision**: The precision with which improvement in performance is measured. Defaults to 0.001     
* **patience**: The number of consecutive epochs without improvement before early stop. Defaults to 5     
* **verbose**: Boolean parameter. If true, statistics are printed to standard out at designated points in the training process, specified by the checkpoint parameter. Defaults to False.      
* **checkpoint**: The frequency in epochs in which statistics are reported if verbose is True.    
* **name**: String indicating the model name      
* **seed**: Seed for reproducibility     

The attributes are:

* **coef_**: The coefficients learned by the model          
* **intercept_**: The intercept learned by the model    
* **epochs_**: The total number of epochs executed       

The class exposes the following public methods:    

* **_init_**: Instantiates an object of the Estimator class            
* **fit(X,y)**: Learns the parameters that minimize the cost function 
* **predict(X)**: Renders a prediction given the current (or final) parameters     
* **score(X,y)**: Computes a score for a designated metric, based upon learned parameters     
* **summary()**: Summarizes key model statistics     

All methods (private and public) are summarized in `r kfigr::figr(label = "estimator_methods", prefix = TRUE, link = TRUE, type="Table")`.

```{python estimator_class_methods, eval=T, echo=F}
df = pd.read_excel("./notes/estimator class.xlsx", sheet_name="methods")
```

`r kfigr::figr(label = "estimator_methods", prefix = TRUE, link = TRUE, type="Table")`: Estimator Methods
```{r estimator_class_methods_table, eval=T, echo=F, results='asis'}
kable(py$df) %>%
  kable_styling(bootstrap_options=c("striped", "hover", "condensed"), full_width=T)
```

With that, let's build our Estimator class.

#### Preliminaries
Let's start by importing necessary modules and defining our class.
```{python preliminaries, eval=F, echo=T, code=readLines('../gradient_descent/v1_linear_regression/estimator.py')[5:22]}
```
The Estimator class is inherited from scikit-learn's BaseEstimator and RegressorMixin classes. They provide methods that allow us to use the [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) and [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) classes.

#### Instantiation
The following method instantiates an object of the Estimator class.
```{python instantiation, eval=F, echo=T, code=readLines('../gradient_descent/v1_linear_regression/estimator.py')[23:62]}
```
#### Validation
Let's include a couple of methods to validate the parameters and data. 
```{python validation, eval=F, echo=T, code=readLines('../gradient_descent/v1_linear_regression/estimator.py')[82:132]}
```

#### Prepare Data
This method creates the design matrix for the input $X$ by adding a vector of ones in the $x_0$ position. This allows us to simplify the prediction calculation to the dot product of $X$ and the parameters $\theta$ without dealing with a separate bias parameter.
```{python prepare_data, eval=F, echo=T, code=readLines('../gradient_descent/v1_linear_regression/estimator.py')[133:144]}
```

#### Early Stopping
Early stopping is a type of regularization used to avoid overfitting when training using methods such as Gradient Descent. Gradient Descent attempts to update the parameters $\theta$ to make it a better fit to the training data with each iteration. This improves the model's performance on test data up to a point. Past that point, improving the performance on the training set comes at the expense of increased generalization error.

We will be implementing a 'default' early stopping rule guided by the following parameters:     

* **early_stop**: Boolean. If True early stopping is based upon performance on the validation set. Otherwise, early stopping is based upon training set performance.   
* **metric**: String indicating the metric used to evaluate performance on the validation and test sets.
* **val_size**: The proportion of the training set to allocate to the validation set.     
* **precision**: The relative improvement threshold by which improvement is measured.     
* **patience**: The number of consecutive epochs with no improvement to wait before early stopping.   

Gradient Descent stops when performance has not improved by at least 'precision'*prior performance for 'patience' consecutive epochs. To evaluate performance we will be monitoring one of the following metrics:     

* training cost    
* training score   
* validation cost    
* validation score  

The following table specifies the metric monitored, based upon values of the above referenced parameters.

```{python early_stopping, eval=T, echo=F}
df = pd.read_excel("./notes/early_stopping.xlsx")
```

`r kfigr::figr(label = "early_stopping_table", prefix = TRUE, link = TRUE, type="Table")`: Early Stopping Performance Measures
```{r early_stopping_table, eval=T, echo=F, results='asis'}
kable(py$df) %>%
  kable_styling(bootstrap_options=c("striped", "hover", "condensed"), full_width=T)
```

If early_stop is True and a validation set has been allocated, we will monitor validation performance. The specific metric to be monitored is based upon the metric parameter.  If a metric has been specified, validation score will be monitored.  Otherwise, performance will be based upon the validation cost.  

On the other hand, if early_stop is False, early stopping will be based upon training set performance. If a metric has been provided, performance will be based upon training set score for the provided metric. Otherwise, early stopping will be based upon training set cost. 

The following abstract and concrete callbacks cover early stopping. 
```{python early_stopping_improvement, eval=F, echo=T, code=readLines('../gradient_descent/v1_linear_regression/early_stop.py')[5:158]}
```

There are six attributes:     

* **metric**: One of 'train_cost', 'train_score', 'val_cost', or 'val_score'.
The 'on_train_begin' initializes best performance, an evaluation function and the precision factor. These values are based upon the metric. If the metric, such as accuracy, improves as it increases, best performance at initialization is $-\infty$, the evaluation function is np.greater, and the precision factor is $+1$. If, on the other hand, the metric improves as it decreases, like mean squared error, the best performance is initialized at $-\infty$, the evaluation function np.less and the precision factor is $-1$. 

At the end of each epoch, the performance is evaluated vis-a-vis the prior epoch's performance. If there has been no improvement in 'patience' epochs, the converged flag on the model is set to True, and training stops.



##### Get Convergence Monitor   
This method instantiates the early stopping callback based upon the parameters as outlined in `r kfigr::figr(label = "early_stopping_table", prefix = TRUE, link = TRUE, type="Table")`.

```{python get_convergence_monitor, eval=F, echo=T, code=readLines('../gradient_descent/v1_linear_regression/estimator.py')[175:198]}
```


#### Compile
The estimator will be obtaining the cost (and gradient) and scoring functions from Cost and Metrics class factories. The selection of cost and and scoring functions will be delegated to subclasses (such as Linear Regression), via abstract methods which will request the functions appropriate to the task. 
```{python compile, eval=F, echo=T, code=readLines('../gradient_descent/v1_linear_regression/estimator.py')[163:174]}
```

The compile method is responsible for obtaining references to external class instances.

```{python compile, eval=F, echo=T, code=readLines('../gradient_descent/v1_linear_regression/estimator.py')[199:204]}
```

#### Initialize Weights
Here, we randomly initialize the parameters where $\theta\sim N(0,1)$.
```{python init_weights, eval=F, echo=T, code=readLines('../gradient_descent/v1_linear_regression/estimator.py')[221:234]}
```

#### Set Learning Rate  
This method has been created to accommodate learning rate schedules (subject of another post). 
```{python get_convergence_monitor, eval=F, echo=T, code=readLines('../gradient_descent/v1_linear_regression/estimator.py')[235:240]}
```

#### Initialize Callbacks
Here, we initialize the History and Progress callbacks.
```{python init_weights, eval=F, echo=T, code=readLines('../gradient_descent/v1_linear_regression/estimator.py')[205:219]}
```
The callbacks are added to the CallbackList object, then each callback is initialized with the parameters and a reference to the model.

#### Set Parameters and Name
In order to leverage scikit-learn's [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) and [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) classes, a set_params method is required to set the parameters of the estimator.  
```{python set_params, eval=F, echo=T, code=readLines('../gradient_descent/v1_linear_regression/estimator.py')[76:81]}
```

In addition, let's include a couple of methods that will automatically assign a name to the model for reporting purposes. The name will take the format task + with + algorithm, e.g. Linear Regression with Stochastic Gradient Descent. Subclasses will specify the task.
```{python set_name, eval=F, echo=T, code=readLines('../gradient_descent/v1_linear_regression/estimator.py')[63:75]}
```

#### Evaluate Epoch
Performance statistics are computed at the end of each epoch.
```{python eval_epoch, eval=F, echo=T, code=readLines('../gradient_descent/v1_linear_regression/estimator.py')[145:161]}
```
#### Begin Training
This method performs all tasks required at the beginning of training. 
```{python begin_training, eval=F, echo=T, code=readLines('../gradient_descent/v1_linear_regression/estimator.py')[241:253]}
```
As indicated, this method will call methods that validate the parameters and the data, prepares the design matrix, initializes the weights and callbacks, and compiles the cost, scoring, and convergence monitoring functions.

#### End Training
Here, we call the on_train_end() methods on the callbacks and save the epochs, coefficient and intercept attributes.
```{python end_training, eval=F, echo=T, code=readLines('../gradient_descent/v1_linear_regression/estimator.py')[255:261]}
```

#### Begin Epoch
This method shuffles the data and calls the 'on_epoch_begin' method on the callbacks.
```{python begin_epoch, eval=F, echo=T, code=readLines('../gradient_descent/v1_linear_regression/estimator.py')[262:269]}
```

#### End Epoch
Here, we format all the data that we would like to track for each epoch. This obtains the training statistics via the '_evaluate_epoch' method. Finally, the 'on_epoch_end' methods are called on the callbacks.
```{python end_epoch, eval=F, echo=T, code=readLines('../gradient_descent/v1_linear_regression/estimator.py')[270:281]}
```

#### Begin Batch
Here, we invoke the 'on_batch_begin' methods on the callbacks.
```{python begin_batch, eval=F, echo=T, code=readLines('../gradient_descent/v1_linear_regression/estimator.py')[282:285]}
```

#### End Batch
Correspondingly, we invoke the 'on_batch_end' methods on the callbacks.
```{python end_batch, eval=F, echo=T, code=readLines('../gradient_descent/v1_linear_regression/estimator.py')[286:288]}
```

#### Fit
This is the driver method for the estimator. It begins by calling the 'begin_training' method, then iterates through the optimization process for a predetermined number of epochs or until convergence is detected. The batch iterator will return batches of the designated batch_size; however if batch_size is None, the entire training set is returned in a single batch. 

Each iteration:  

* computes the predictions for the batch    
* calculates the cost so that it can be tracked at the batch level    
* determines the gradient    
* updates the parameters as a function of the gradient and the learning rate $\eta$    

```{python fit, eval=F, echo=T, code=readLines('../gradient_descent/v1_linear_regression/estimator.py')[289:335]}
```

#### Prediction
Here we have the methods used for prediction. The first, 'linear_prediction' computes the prediction as a linear combination of the inputs $X$ and the parameters $\theta$. The '_predict' and 'predict' methods are defined by subclasses to accommodate tasks requiring non-linear transformations of the linear predictions, e.g. sigmoid or softmax.

The 'linear_prediction' method is used in two contexts.   

1. Training Predictions - It computes a prediction on the formatted design matrix $X$ and takes the form $\hat{y}=X\cdot\theta$
2. Test Set Predictions - Test sets are distinguished by the number of features in the dataset vis-a-vis the dimension of the parameters $\theta$. If the number of features doesn't match the length of the parameters $\theta$, it is assumed to be a validation or test set. Here the prediction takes the form $\hat{y}=\theta_0 + X\cdot\theta_{\{1..n\}}$.
```{python predictions, eval=F, echo=T, code=readLines('../gradient_descent/v1_linear_regression/estimator.py')[336:354]}
```

#### Score
The score function computes score for the designated metric. This abstract method delegates to subclasses in order to distinguish regression from classification metrics.
```{python score, eval=F, echo=T, code=readLines('../gradient_descent/v1_linear_regression/estimator.py')[355:358]}
```

#### Summary
Lastly, we have a method that summarizes the training process using the History object.
```{python summary, eval=F, echo=T, code=readLines('../gradient_descent/v1_linear_regression/estimator.py')[359:361]}
```

Now, we can move onto the Regression and Linear Regression classes.

### Regression    
The Regression class inherits from the Estimator class.  Looking ahead, we will be building, not only the Linear Regression capability, but Lasso, Ridge, and ElasticNet Regression facilities as well. Here, we build those methods common to all regression classes.
```{python regression, eval=F, echo=T, code=readLines('../gradient_descent/v1_linear_regression/regression.py')[5:113]}
```

Here, we've:   
* obtained the cost function from the RegressionCostFactory    
* designated the scoring function from the RegressionMetricFactory      
* defined an private and public predict method that calls the 'linear_prediction' method in the base class   
* specified a scoring function

### Linear Regression
The Linear Regression class inherits virtually all the required functionality from the Regression class. All that is remaining is to designate the name of the model for reporting purposes. 

```{python linear_regression, eval=F, echo=T, code=readLines('../gradient_descent/v1_linear_regression/regression.py')[119:243]}
```

