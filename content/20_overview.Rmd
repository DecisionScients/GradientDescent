<script type="text/x-mathjax-config">
MathJax.Hub.Config({
TeX: {
equationNumbers: {
autoNumber: "all",
formatNumber: function (n) {return ''+n}
}
}});</script>

# Gradient Descent
<center>
![Gradient Descent](../content/figures/gradient_descent.jpg)
</center>
<center> `r kfigr::figr(label = "linear_search", prefix = TRUE, link = TRUE, type="Figure")`: Gradient Descent </center>


Gradient descent is among *the* most important algorithms in machine learning and is the defacto standard for optimizing deep learning networks. First introduced by Louis Augustin Cauchy in 1847 [@Cauchy1847Methode], Gradient Descent has evolved into a family of optimization algorithms and is now the workhorse behind state-of-the-art deep learning frameworks such as Keras [@chollet2015keras], TensorFlow [@tensorflow2015-whitepaper], Caffe [@jia2014caffe], Lasagne [@lasagne], and PyTorch [@paszke2017automatic], to name a few. 

This is section 1st of a _-part series, Gradient Descent from Scratch. The goal of the series is to:    
* Examine the fundamental principles of Gradient Descent,     
* Build a Gradient Descent package in Python from scratch, and    
* Explore the Gradient Descent's empirical behaviour through experimentation.  

Our objective here is to explore the mathematical intuition behind Gradient Descent in order to understand how and why it really works. 

So, let's set the context with the machine learning model representation.
<center>
![Model Representation](../content/figures/model_representation.png)
</center>
<center> `r kfigr::figr(label = "linear_search", prefix = TRUE, link = TRUE, type="Figure")`: Model Representation </center>


The supervised learning goal is, given a training set, to learn a function $h:\mathcal{X}\to\mathcal{Y}$ so that $h(x)$, parameterized by $\Theta$, is a "good" predictor for the corresponding value of $y$. For historical reasons, known only to Andrew Ng, this function is called $h$, the hypothesis function. Next, we have an objective or cost function that quantifies a penalty $J(\theta)$ for predicting a value of $\hat{y}$ when the true value is $y$. Gradient Descent seeks to **minimize** $J(\theta)$ by computing the gradient of the cost function $\nabla J(\theta)$, then updating the parameters $\theta$ according to the magnitude of the gradients. Once the parameters are updated, the objective function computes the associated costs, $J(\theta)$ and the process repeats until the gradient has vanished and can no longer be minimized. 




That's pretty much it. With that, let's formally define Gradient Descent.

> Gradient descent is a first-order iterative optimization algorithm for finding the minimum of a real-valued, unconstrained, differentiable objective function: $\mathbb{R}^n \to \mathbb{R}$. Parameterized by $\theta \in \mathbb{R}^n$, Gradient Descent iteratively updates the parameters in the direction opposite to the gradient of the objective function $\nabla_\theta J(\theta)$.

If that definition works for you, skip ahead to the next section. Otherwise, if you are like me (and I know I am), perhaps it would be useful to develop these ideas from fundamentals. The next sections analyze how to:

* Minimize Linear Functions    
* Minimize Univariate Differentiable Functions     
* Minimize Multivariate Differentiable Functions    

We'll inject the underpinning mathematics along the way.

## Minimize Linear Functions
Let's now consider the linear function of the form:
$$f(x)=a^Tx+b$$
where:     

* $x \in \mathbb{R}^n$ is an $n$-vector,    
* $a \in \mathbb{R}^n$ is an $n$-vector, 
* $b \in \mathbb{R}$ is a scalar,      
* $n>1$     

The goal is: $\text{min}\space f(x)$, with no constraints on $x$. Let's do a little experiment. The animation in `r kfigr::figr(label = "linear_search", prefix = TRUE, link = TRUE, type="Figure")` displays the surface for $f(x)=a^Tx + b$, where:    


$$a=\begin{bmatrix}-5\\-3\end{bmatrix},\text{and}\: b=4$$ 

```{python linear_search, eval=F, code=readLines('../content/code/linear_search.py')}
```

```{r linear_search_r, echo=FALSE}
htmltools::includeHTML("./content/figures/linear_search.html")
```

<center>`r kfigr::figr(label = "linear_search", prefix = TRUE, link = TRUE, type="Figure")`: Minimizing a linear function $f(x)\in\mathbb{R}^2$ </center>


We are currently at point $A$, $x^T=[0,0]$, and we wish to take a step of one unit of Euclidean distance in the direction that minimizes $f(x)$ the most. Adjust the slider to select a direction in degrees for a unit vector $\vec{u}$ and observe the value of $f(\vec{x}+\vec{u})$ which will appear once you've changed the direction using the slider widget. Which direction minimizes $f(x)$ the most? When you find it, note the coordinates of $\vec{u}$.

We can solve for $\vec{u}$ that minimizes $f(x)$ the most by using a bit of linear algebra. Note that moving from $x$ one unit of distance in the direction of $u$ changes our objective function as follows:

$$
\begin{align}
a^Tx + b & \to a^T(x+u)+b \\
& = a^Tx + b+a^Tu \\
& = a^Tx + b + \|a\|_2\space\|u\|_2\space \text{cos}\space\theta
\end{align}
$$
where $\theta$ denotes the angle between the vectors $a$ and $u$. To minimize this function, we must make $\|a\|_2\space\|u\|_2\space\text{cos}\space\theta$ as small as possible. This occurs when we chose $u$, such that  $\text{cos}\space\theta=-1$, which happens to be the opposite direction of $a$. Since $a$ is the direction of steepest ascent, $-a$, the direction that minimizes $f$ the most, is the direction of **steepest descent**. 

Let's solve for $u$ analytical, then compare it to our experimental result above. The formula for computing $\vec{u}$ in the opposite direction of $\vec{a}$ is given by:
$$\vec{u}=-\frac{1}{||\vec{a}||}\vec{a}$$
Let's solve for $\vec{u}$.
```{python u, eval=T, echo=T}
a = np.array([-5,-3])
x = np.array([0,0])
b = 4
u = np.round(-a / np.sqrt(np.sum(a**2)),4)
```

The unit vector $\vec{u}$ that minimizes $f(x)$ the most is [`r py$u[1]`,`r py$u[2]`]. Now, we can solve for $f(\vec{x}+\vec{u})$ as follows:
```{python f_u, eval=T, echo=T}
def f(x, a, b):
    return np.dot(a.T, x) + b
z = np.round(f(x+u, a, b), 4)
```

The minimum of $f(\vec{x}+\vec{u})$ is about `r py$z`, which occurs at about 31 degrees. How close did you get?   

So, the main takeway is this. For linear functions, we minimize $f$ the most by moving in the direction of the **steepest descent**.

## Minimize Univariate Differentiable Function
First, let's define our terms.  A univariate function $f(x):\mathbb{R}\to\mathbb{R}$ is a function with a single $x$ that maps to a single real-valued output, $f(x)$.

![](./figures/differentiability.jpg)

<center>`r kfigr::figr(label = "differentiability", prefix = TRUE, link = TRUE, type="Figure")`: Differentiability</center>

As shown in the top row of `r kfigr::figr(label = "differentiability", prefix = TRUE, link = TRUE, type="Figure")`, a function that is *differentiable* at a point, say $x_0$ is one whose derivative exists at $x_0$. It has a (non-vertical) tangent line at that point, it is relatively smooth, and does not contain any breaks, bends, or cusps. A differentiable function has a derivative at all points $x\in \mathbb{R}$. (There are extensions of Gradient Descent that relax the differentiability assumption, but those are a subject for another post.)

Now, consider a relatively simple function $f(x)=x^2$ - a univariate differentiable function. 
```{python univariate, eval=F, code=readLines('../content/code/univariate.py')}
```

```{r convex_univariate_r, echo=FALSE}
htmltools::includeHTML("./content/figures/convex.html")
```
<center>`r kfigr::figr(label = "convex_univariate", prefix = TRUE, link = TRUE, type="Figure")`: Convex univariate objective function</center>

So, how would we minimize $f(x)$? Well, if we were at point $x_0$ on `r kfigr::figr(label = "convex_univariate", prefix = TRUE, link = TRUE, type="Figure")`, we observe that the graph is increasing to the left and decreasing to the right, so we move to the right. Starting at $x_1$, we move to the left to make $f$ as small as possible. In either case, we eventually arrive at the bottom of the basin, which happens to be the *global* minimum for $f$. 

Not all objective functions are as agreeable as `r kfigr::figr(label = "convex_univariate", prefix = TRUE, link = TRUE, type="Figure")`. Consider `r kfigr::figr(label = "nonconvex_univariate", prefix = TRUE, link = TRUE, type="Figure")`. 
```{r nonconvex_univariate_r, echo=FALSE}
htmltools::includeHTML("./content/figures/nonconvex.html")
```
<center>`r kfigr::figr(label = "nonconvex_univariate", prefix = TRUE, link = TRUE, type="Figure")`: Non-convex univariate objective function</center>

If we start at $x_0$, we end up in the left basin, which happens to be the global minimum; however, If we start at $x_1$, we eventually arrive at a sub-optimal *local* minimum. 

A key takeaway: Gradient descent is guaranteed to converge to a global minimum with convex objective functions. On the other hand, non-convex functions have multiple minima. As such, there is no guarantee that Gradient Descent will converge to the global minimum for non-convex objective functions. 

So, let's formalize the process of minimizing a univariate differentiable function. The basic algorithm for minimizing a univariate differentiable function $f(x):\mathbb{R}\to\mathbb{R}$ uses the **derivative**, $f^\prime(x)$, to determine the direction to move, and is given by:

While $f^\prime(x)\ne0$

  a. if $f^\prime(x)>0$, $f$ is increasing, move a $x$ a little to the left;
  b. if $f^\prime(x)<0$, $f$ is decreasing, move a $x$ a little to the right;

So, what is the derivative, and why do we care? Great questions.

### Mathematical Intuition
#### The Derivative
I'll answer the second question first. Why do we care? Well, the key to minimizing *any* differentiable function is really rather simple.

> The key to minimizing any differentiable function, is to reduce it to a linear function.

This may sound glib, especially given the crazy forms differentiable functions can take. But calculus... uhh...uhh... finds a way! 

It turns out that any function differentiable at a point can be locally approximated at that point by a linear function. Once we have a local *affine* approximation of our function, we can use properties of differentiation to optimize the true function $f$. 

We care about the derivative of a univariate function $f$ at a point $x$ because it gives us the rate of change for the tangent line, the *best* local affine approximation of $f$ at $x$. 

So, what is the derivative?

A function $f$ is said to be differentiable at $x$ if the following limit exists:
$$\displaystyle{\lim_{h \to0}}\Big[\frac{f(x+h)-f(x)}{h}\Big].$$
This limit is called the derivative of $f(x)$, denoted as $f^\prime(x)$. 
```{python derivative, eval=F, code=readLines('../content/code/derivative.py')}
```

```{r derivative_as_limit, echo=FALSE}
htmltools::includeHTML("./content/figures/derivative.html")
```
<center>`r kfigr::figr(label = "derivative_as_limit", prefix = TRUE, link = TRUE, type="Figure")`: Derivative as a Limit</center>

To develop the intuition behind the derivative as a limit, consider the function $f(x)=x^2$ as specified in `r kfigr::figr(label = "derivative_as_limit", prefix = TRUE, link = TRUE, type="Figure")`. We have two points: $P$ at $f(x)$ and $Q$ at $f(x+h)$, and a secant line connecting them. Press play to move point $Q$ along the curve towards point $P$ as $h$ diminishes. 

What do we notice? First, the secant changes direction as $Q$ approaches point $P$. As the absolute value of $h$ decreases, the value of $x+h$ approaches $x$, and the value of $f(x+h)$ approaches $f(x)$. It should be intuitively clear that the tangent line is the limit approached by the secant as $h\to0$. 

The slope of the secant line connecting the points ($x+h,f(x+h)$) and ($x,f(x)$) is the difference in the $y$ values over the difference between the $x$ values. This **difference quotient** becomes:
$$m=\frac{f(x+h)-f(x)}{h}$$
Hence small values of $h$ that approach zero render secant lines that are increasingly better approximations of the tangent line to $f(x)$ at $x$. Since the limiting position as $h \to 0$ of the secant line is the tangent line, the difference quotient as $h \to 0$, if it exists, will be the slope of the tangent line at ($x, f(x)$). This limit is defined as the **derivative** of the function $f$ at $x$:
$$f^\prime(x)=\lim_{h\to0}\frac{f(x+h)-f(x)}{h}$$
Fair enough, but we claimed that the tangent line was the *best* local affine approximation of $f$ at $x$. Let's convince ourselves of that.

#### Tangent Line: Best Local Affine Approximation
##### Intuitive Interpretation
Recall calculus promises that, for a function $f(x)$ and a point suitably close to $x$, one can pretend that the true function $f$ is just a linear function at $f(x)$. This leads us to the most salient interpretation of the tangent line, and it is this.

> The tangent line of a univariate, differentiable function $f$ at point $x$ is the **best** local affine approximation of $f$ at $x$. 

If we were to zoom into the graph of a function $f$ at a point $x$ using a high-powered microscope (or look at it over a small enough interval), the curve of the function would appear to become increasingly straight. In fact, if we observe $f$ at the intersection of the tangent line, the curve and the tangent line would become qualitatively indistinguishable for all points *near* the point of intersection. 

##### Analytical Interpretation
Let's first derive the equation for the tangent line. Recall, we have the following difference quotient.
$$m=\frac{f(x+h)-f(x)}{h}$$
Using derivatives, we've shown that the slope $m$ approaches the slope of the tangent line $f^\prime(x)$ as $h\to0$. Substituting $f^\prime(x)$ for $m$ and rearranging into point slope form yields:
$$y=f(x)+f^\prime(x)(h).$$
Viola, the equation for the tangent line. The derivative is its slope. Now, consider the following Taylor series at $x$:

$$f(x+h)=\underbrace{f(x)+hf^\prime(x)}_\text{Tangent Line}+h^2f^{\prime\prime}(x)/2+\cdots$$
If you are not familiar with the Taylor series, it is just a representation of a function in terms of an *infinite sum* of the function's derivatives at a single point. Using Taylor's theorem, we can do two useful things: 

  1. approximate a function by using a *finite* number of terms of the Taylor series, and 
  2. quantify an estimate of the *error* introduced by the use of the Taylor approximation
  
Using the first three components of the Taylor series means that, for small $h$:
$$f(x+h)\approx \underbrace{f(x)+hf^\prime(x)}_\text{Tangent Line}+h^2f^{\prime\prime}(x)/2\label{eq:taylor}$$

We can quantify the error as:
$$E(x,h)=f(x+h)-(f(x)+hf^\prime(x))\approx h^2f^{\prime\prime}(x)/2$$
Now consider any other line through $(x,f(x))$ with slope $s$, with $s\ne f^\prime(x)$. At $x+h$, its value is $f(x)+hs$, and its error, $e(x,h,s)=f(x+h)-(f(x)+hs).$

Since $f(x+h)-f(x)\approx hf^\prime(x)+h^2f^{\prime\prime}(x)/2$, we can compute the associated error as: 
$$
\begin{equation}\begin{split}e(x,h,s) & =f(x+h)-(f(x)+hs) \\
& \approx hf^\prime(x)+h^2f^{\prime\prime}(x)/2-hs\\
&=h(f^\prime(x)-s)+h^2f^{\prime\prime}(x)/2
\end{split}
\end{equation},
$$
so that:
$$\frac{E(x,h)}{e(x,h,s)}\approx\frac{h^2f^{\prime\prime}(x)/2}{h(f^\prime(x)-s)+h^2f^{\prime\prime}(x)/2}=\frac{hf^{\prime\prime}(x)/2}{f^\prime(x)-s+hf^{\prime\prime}(x)/2}.$$
Since $s\ne f^\prime(x)$, as $h \to 0$, the numerator of the ratio of errors goes to zero while the denominator stays bounded away from zero.

Because, the error of the tangent lines goes to zero faster than the error of any other line through the point, the tangent line is the **best local affine approximation** to $f$ at $x$ [@Bivens1986]. 

Before we move on to general differentiable functions, let's review a method for computing derivatives that doesn't involve limits.

#### Rules of Differentiation
Differentiation is the process of finding the derivative of a differentiable function. There are two approaches to differentiation. One approach is to use the geometric definition, but this can be quite a slow and cumbersome process. Fortunately, we have a small number of rules that enable us to differentiate large classes of functions quickly.

For the following exhibit, let, $u=x^2$, $v=4x^3$, and $y=u^9$

<center>`r kfigr::figr(label = "differential", prefix = TRUE, link = TRUE, type="Table")`: Differentiation Rules</center>
```{r differential, results='asis'}
tbl <- "
|     Rule         |  Definition             |                    Example                   |
|------------------|:-----------------------:|:--------------------------------------------:|
| Constants Rule   | $\\frac{d}{dx}c=0$      |  $\\frac{d}{dx}5=0$|
| Power Rule   | $\\frac{d}{dx}x^n=nx^{n-1}$ |  $\\frac{d}{dx}v=12x^2$|
| Constant Factor Rule | $\\frac{d}{dx}(cu)=c\\frac{du}{dx}$| $\\frac{d}{dx}3x^2=3\\frac{d}{dx}x^2=6x$|
| Sum Rule | $\\frac{d}{dx}(u+v)=\\frac{du}{dx}+\\frac{dv}{dx}$ | $\\frac{d}{dx}(x^2+4x^3)=\\frac{d}{dx}x^2+\\frac{d}{dx}4x^3=2x+12x^2$|
| Subtraction Rule |$\\frac{d}{dx}(u-v)=\\frac{du}{dx}-\\frac{dv}{dx}$ | $\\frac{d}{dx}(x^2-4x^3)=\\frac{d}{dx}x^2-\\frac{d}{dx}4x^3=2x-12x^2$|
| Product Rule | $\\frac{d}{dx}(uv)=u\\frac{dv}{dx}+v\\frac{du}{dx}$ | $\\frac{d}{dx}(x^2\\times 4x^3)=x^2 \\times 12x^2 + 4x^3 \\times 2x=20x^4$ |
| Quotient Rule | $\\frac{d}{dx}(\\frac{u}{v})=\\frac{v\\frac{du}{dx}-u\\frac{dv}{dx}}{v^2}$ for $v\\ne 0$ | $\\frac{d}{dx}(\\frac{u}{v})=\\frac{4x^3\\times 2x - x^2\\times 12x^2}{16x^6}=-\\frac{1}{4^2}$|
| Chain Rule | $\\frac{dy}{dx}=\\frac{dy}{du}\\times \\frac{du}{dx}$ | $\\frac{dy}{dx} = 9u^8 \\times 2x = 9(x^2)^8 \\times 2x=18x^{17}$|

"
cat(tbl)
```

For practice, let's run through an example. 
$$\frac{d}{dx}\bigg[\frac{2x+4}{3x-1}\bigg]^3$$
1. Let $y=f(u)=u^3$ and $u=g(x)=\frac{2x+4}{3x-1}$ by the chain rule.    
2. Then $f^\prime(u)=3u^2$ by the power rule.    
3. Applying the quotient rule to obtain $g^\prime(x)$, we have    
$$\frac{d}{dx}\bigg[\frac{2x+4}{3x-1}\bigg]= \frac{(3x-1)(2)-(2x+4)(3)}{(3x-1)^2}=\frac{-14}{(3x-1)^2}$$
4. Hence, by the chain rule we have:  
$$
\begin{align}
F^\prime(x) & =f^\prime(g(x))\times g^\prime(x) \\
& = 3\bigg(\frac{2x+4}{3x-1}\bigg)^2 \times \frac{-14}{(3x-1)^2} \\
& = \frac{-42(2x+4)^2}{(3x-1)^4}
\end{align}
$$ 
As we've seen the rules of differentiation can allow us to find the derivatives relatively quickly without dealing with limits.  

### Key Takeaways: Minimize Univariate Function
So, we've shown that:     

1. we can approximate a univariate differentiable function using affine approximation      
2. the tangent line to a univariate function $f$ at a point $x$ is the best local affine approximation of $f$ at a $x$    
3. the derivative, denoted $f^\prime(x)$, is the slope of the tangent line of $f$ at $x$     
4. to minimize a univariate differentiable function $f$:       
  While $f^\prime(x)\ne0$      
    a. if $f^\prime(x)>0$, $f$ is increasing, move a $x$ a little to the left;            
    b. if $f^\prime(x)<0$, $f$ is decreasing, move a $x$ a little to the right;                  
    
Now, let's move on to general multivariate differentiable functions, ya know, the ones we actually care about.

## Minimize General Multivariate Differentiable Functions
Multivariate functions are those with $n>1$ dimensions or variables. When $n>1$, our optimization challenge gets complicated because we have an infinite number of directions in which to move, not just two as was the case with the univariate function. But everything we've said extends to an arbitrary number of $n$ dimensions. For instance, the Taylor expansion (Eq. $\ref{eq:taylor}$) remains valid in higher dimensions, just with the derivative replaced with the higher dimension analog, the *gradient*.

Let's revisit some multivariate calculus, so that we are clear on the gradient and what it tells us about multivariate functions.

### Mathematical Intuition
So, what is a gradient? The gradient of a scaler-valued, multivariate, differentiable function $f:\mathbb{R}^n\to\mathbb{R}$ is a vector-valued function $\nabla f:\mathbb{R}^n\to\mathbb{R}^n$ whose value at a point $p$ is the vector whose components are the partial derivatives of $f$ at $p$ [@wiki:gradient_descent].

If, at a point $p$, the gradient of the function is not the zero vector, the gradient points in the direction of steepest ascent of the function at $p$. Its magnitude is the rate at which the function is increasing in that direction. Alternatively, a gradient equal to the zero vector indicates a *stationary point* in which the function is neither increasing or decreasing in any direction.

To fully understand the gradient, we must understand the partial derivative.

#### Partial Derivative
The partial derivative of a multivariable function tells us know how much the function changes as you tweak just one of the variables in its input. 

##### Visualizing Partial Derivatives


```{python partial, eval=F, code=readLines('../content/code/partial.py')}
```

To graphically illustrate, let $f(x,y) = x^2+xy-y^2$ and consider the task of finding the partial derivative of $f(x,y)$ with respect to $x$ and $y$ at a $f(-2,-2)$. We can visualize the surface plot for the equation as follows. 

![](./figures/multivariable_function.png)

<center>`r kfigr::figr(label = "multivariable", prefix = TRUE, link = TRUE, type="Figure")`: Surface plot for $f(x,y) = x^2+xy-y^2$</center>

First, let's consider $\frac{\partial{f}}{\partial{x}}$, the partial derivative of $f$ with respect to $x$ This means that we treat $y$ as constant and compute the effect of small changes in $x$ on $f$. 

One way to visualize this is to inject a plane into the surface plot at some constant value for $y$.  Since we are seek the $\frac{\partial{f}}{\partial{x}}$ at $(-2,-2)$, let's plot a plane through the surface at $y=-2$.

![](./figures/partial_wrt_x.png)

<center>`r kfigr::figr(label = "partial_x", prefix = TRUE, link = TRUE, type="Figure")`: Surface plot for $\frac{\partial{f}}{\partial{x}}$</center>

The plane in `r kfigr::figr(label = "partial_x", prefix = TRUE, link = TRUE, type="Figure")`, provides a cross-section into $f$, revealing the behavior of the function for $y=-2$. The red line is the tangent line to $f$ at $(-2,-2)$ in the direction of the $x$ axis. The slope of the tangent line is  $\frac{\partial{f}}{\partial{x}}$, the partial derivative of $f$ with respect to $x$ at $(-2,-2)$.

Similarly, the graph of the $\frac{\partial{f}}{\partial{y}}$, the partial derivative of $f$ with respect to $y$ at $(-2,-2)$ is obtained by taking the cross-section into $f(x,y)$ at some constant value of $x$. This time we will take the cross section at $x=-2$.

![](./figures/partial_wrt_y.png)

<center>`r kfigr::figr(label = "partial_y", prefix = TRUE, link = TRUE, type="Figure")`: Surface plot for $\frac{\partial{f}}{\partial{y}}$</center>

The red tangent line in `r kfigr::figr(label = "partial_y", prefix = TRUE, link = TRUE, type="Figure")`, is the linear approximation of $f$ in the direction of the $y$ axis.  The slope of this tangent line is $\frac{\partial{f}}{\partial{y}}$, the partial derivative with respect to $y$ at $(-2,-2)$.

Now that we have a graphical intuition, let's review how the partial derivative of a multivariable function is computed.

##### Computing the Partial Derivative
Consider our function:
$$f(x,y) = x^2+xy-y^2$$
How would we evaluate $\frac{\partial{f}}{\partial{x}}$, the partial derivative with respect to $x$ at input $(-2, -2)$? 

The good news is that we use the same mechanics as an ordinary derivative. Since we only care about movement in the $x-$direction, we might as well treat the $y-$value as a constant.  In fact, we may can just plug in $y=-2$ into $f$ before computing any derivatives:
$$f(x,-2) = x^2+x(-2)-(-2)^2=x^2-2x+4$$
Now, $\frac{\partial{f}}{\partial{x}}$, the partial derivative with respect to $x$ is just an ordinary univarite derivative.
$$\frac{\partial}{\partial{x}}f(x,-2)=2x-2$$
Similarly, we can find $\frac{\partial{f}}{\partial{y}}$, the partial derivative with respect to $y$ by plugging in $x=-2$ into $f$:
$$f(-2,y) = (-2)^2+(-2)y-y^2=4-2y-y^2$$
Likewise, $\frac{\partial{f}}{\partial{y}}$, the partial derivative with respect to $y$ is just:
$$\frac{\partial}{\partial{y}}f(-2,y)=-2y-2$$
The previous example showed us how to compute the partial derivatives at a specific point. To generalize this, we need a way to compute the rates of change of $f$ near any point $(x,y)$. In other words, we need a new multivariable function which takes any point $(x,y)$ as input and tells us the rate of change of $f$ near that point as we move purely in the $x-$direction for $\frac{\partial{f}}{\partial{x}}$ or $y-$direction for $\frac{\partial{f}}{\partial{y}}$. 

To compute $\frac{\partial{f}}{\partial{x}}$, the partial derivative with respect to x for any $(x,y)$ we treat the $y$ variable as a constant, then take the derivative. Recall that the derivative of a constant is 0. Hence, 
$$\frac{\partial}{\partial{x}}f(x,y)=\frac{\partial}{\partial{x}}x^2+xy-y^2=2x+y.$$
Treating $x$ as a constant, we can compute $\frac{\partial{f}}{\partial{y}}$, the partial derivative with respect to $y$ as:
$$\frac{\partial}{\partial{y}}f(x,y)=\frac{\partial}{\partial{y}}x^2+xy-y^2=x-2y.$$

#### The Gradient
Now that we've explored *partial* derivatives of multivariable functions, you may be wondering what *full* derivative of such a function is.  For **scalar-valued multivariable functions**, the answer is the gradient.

The gradient is the vector-valued, multivariable generalization of the derivative [@gradient]. The gradient of a scalar-valued multivariable function $f(x,y,...)$, denoted as $\nabla{f}$, packages all if its partial derivative information into a vector, e.g.:
$$
\nabla{f} = 
\begin{bmatrix}
\frac{\partial{f}}{\partial{x}} \\
\frac{\partial{f}}{\partial{y}} \\
\vdots
\end{bmatrix}
$$


##### Gradient Example in Two Dimensions
For example, let $f(x,y)=x^2+xy$, then the gradient is defined as:
$$
\nabla{f} = 
\begin{bmatrix}
\frac{\partial{f}}{\partial{x}} \\
\frac{\partial{f}}{\partial{y}} \\
\end{bmatrix}
= 
\begin{bmatrix}
2x+y \\
x \\
\end{bmatrix}
$$

##### Gradient Example in Three Dimensions   
In this example, let $f(x,y,z)=x-xy+z^2$, then the gradient at $f(3,3,3)$ is:
$$
\nabla{f} = 
\begin{bmatrix}
\frac{\partial{f}}{\partial{x}} \\
\frac{\partial{f}}{\partial{y}} \\
\frac{\partial{f}}{\partial{z}} \\
\end{bmatrix}
= 
\begin{bmatrix}
1-y \\
-x \\
2z \\
\end{bmatrix}
= 
\begin{bmatrix}
-2 \\
-3 \\
6 \\
\end{bmatrix}
$$

#### Interpreting the Gradient
The most germain interpretation of the gradient to optimization is that the gradient of a function $f$ is a vector that points in the direction of the **steepest ascent** of $f$ at a point.

To show this, we need to develop the concept of the **directional derivative**. The directional derivative is a generalization of the partial derivative that measures rates of change of a function in *any* arbitrary direction. First, we specify directions as unit vectors whose lengths equal 1. Let $\mathbb{u}$ be such a unit vector, $\|\mathbb{u}\|=1$. Then, let's define the *directional derivative* of $f$ in the direction of $\mathbb{u}$ as being the limit:
$$D_uf(a)=\lim_{h\to0}\frac{f(a+\textit{h}\mathbb{u})-f(a)}{\textit{h}}.$$
This is the rate of change of $f$ as $x\to a$ in the direction of $\mathbb{u}$. As we know, the tangent line is a good approximation of a single variable function at a point. Analogously, a multivariable function $f$, if differentiable, is well approximated by the tangent *hyperplane*.  The linear function $g$ for the tangent hyperplane is given by:
$$g(x)=f(a)+f_{x_1}(a)(x_1-a_1)+\dots +f_{x_n}(a)(x_n-a_n).$$
Therefore,
$$
\begin{align}
D_uf(a) & = \lim_{h\to0}\frac{f(a+hu)-f(a)}{h} \\
& = \lim_{h\to0}\frac{g(a+hu)-f(a)}{h} \\
& = \lim_{h\to0}\frac{f_{x_1}(a)hu_1+f_{x_2}(a)hu_2+\dots+f_{x_n}(a)hu_n}{h} \\
& = f_{x_1}(a)u_1+f_{x_2}(a)u_2+\dots+f_{x_n}(a)u_n,
\end{align}
$$
where $x_i \in x_1,x_2,\cdots ,x_n$ and $f_{x_i}=\frac{\partial{f}}{\partial{x_i}}$. 

Observe that the gradient of $f$ is:    
$$\nabla f =\begin{bmatrix}f_{x_1}(a),\\f_{x_2}(a),\\\dots\\f_{x_n}(a)\end{bmatrix}.$$ 
Hence the directional derivative is simply the dot product of the gradient and the vector $\mathbb{u}$:

$$D_uf(a)=\nabla f(a)\cdot \mathbb{u}.$$

The gradient $\nabla f(a)$ is a vector in a specific direction. Since $\mathbb{u}$ can be a unit vector in any direction, let $\theta$ be the angle between $\nabla f(a)$ and $\mathbb{u}$. Now we can rewrite the directional derivative.
$$D_uf(a)=\nabla f(a)\cdot\mathbb{u}=\|\nabla f(a)\|\|u\|\space cos\space\theta$$

Since $D_uf(a)$ is largest when $cos\space\theta=1$, $\theta$ must be 0. Hence, $u$ points in the direction of the gradient $\nabla f(a)$. So, we conclude that $\nabla f(a)$ points in the direction of the greatest increase of $f$, that is, the direction of the **steepest ascent**.

To minimize a function, Gradient Descent moves in the opposite direction, the direction of steepest descent.

### Gradient Descent Algorithm
We've covered the mathematics behind the gradient and its interpretation as the direction of steepest descent for a multivariate function $f$ at a point $p$.  Now, we are ready to examine the Gradient Descent algorithm. Actually, it's not unlike the basic algorithm used to minimize a univariate differentiable function.  

Consider an objective function $J(\theta): \mathbb{R}^n\to\mathbb{R}$. We minimize $J(\theta)$ using Gradient Descent as follows:

Set learning rate $\alpha$              
Normalize data $X$                
Randomly initialize $\theta$                 
While $\nabla J(\theta) != 0$                  
  $\theta_j := \theta_j - \alpha \nabla J(\theta_j)\space\text{for every }j \in n$                 
  

#### Learning Rate
Each step in Gradient Descent is scaled by a value called the step size, or learning rate $\alpha$. 
<center>
![Learning Rates](../content/figures/learningrates.jpeg)
</center>

<center>Image by [Andrej Karpathy](http://cs231n.github.io/neural-networks-3/)</center>

<center>`r kfigr::figr(label = "partial_x", prefix = TRUE, link = TRUE, type="Figure")`: Learning Rates </center>

The learning rate must be carefully chosen, because if it is too small (blue line) the algorithm will take an excessive amount of time to train. High learning rates (green line), on the other hand, decay the loss faster, but they may cause the algorithm to oscillate around a minimum, resulting in higher loss values. Excessively high learning rates (yellow line) may cause the algorithm to diverge from the minimum all together. 


#### Normalize Data
Gradient Descent is sensitive to varying scales between features. Consider a model to predict housing prices based upon two predictors: the number of rooms $r$ in the range of 1 to 10, and the area $a$ in thousands of feet. The value of $y$ is in 1000's of dollars. 

Since values for $a$ are two order of magnitude greater than the values for $r$, the gradients will vary accordingly. This will result in very large steps in the $a$ direction, overwhelming any progress in the $r$ direction. At best, this will result in poor parameter estimates and at worst, cause the algorithm to fail to converge all together.

This problem is solved by normalizing all the data so that they are on the same scales. In machine learning, t is most common to center and scale the data around a mean $\mu=0$ and unit variance $\sigma=1$.

#### Random Initialization
The Gradient Descent begins by randomly initializing the parameters $\theta$. There are blog posts which survey and contrast various parameter initialization methods for neural networks. Yet, normal random initialization where $\theta \sim \mathcal{N}(\mu=0,\sigma^2=1)$.

#### Parameter Update
Given the gradient $\nabla J(\theta)$, and a suitable learning rate $\alpha$, we are now ready to update our parameters $\theta$ in order to 'move' to the new location which minimizes the cost function the most. We do this by performing the following update to $\theta$:

$$\theta_j=: \theta_j - \alpha \nabla J(\theta_j)\space(\text{for every }j\in n)$$

We simultaneously subtract the gradient multiplied by the learning rate from the current parameters $\theta_j$ to move to the new location. Once the parameters are updated, the objective function is computed, the gradient is calculated for the new location and the process repeats until the gradient becomes the zero vector (or a stopping condition is met).

### Key Takeaways: Minimize Multivariate Differentiable Function
We've covered a lot of territory. Let's summarize the main ideas.

* Gradient descent is an iterative approximation algorithm designed to solve unconstrained and differentiable optimization problems of the form:

$$\text{min}\space f(x)$$
      - where $f(x)$ is a differentiable objective function $f:\mathbb{R^n}\to\mathbb{R}$ defined on $n \ge 1$-dimensional Euclidean space and $x\in\mathbb{R}^n$.

* Objective functions may be convex, with a single global optimum, or non-convex with several local minima and a global minimum. Gradient descent is guaranteed to converge to a global minimum for convex objective functions. There is no such guarantee for non-convex objective functions.       
* We minimize a univariate function by repeatedly taking small steps in the direction opposite to the sign of the derivative of the function.       
* To minimize a multivariate objective function, we move in the direction opposite to the gradient.    
* The partial derivative of a function $f$ with respect to one of its variables tells us how much the function changes as we tweak just one of the variables, leaving the others constant.   
* The directional derivative tells us the rate of change of a function in any arbitrary direction.    
* The gradient is the full derivative of a scalar-valued multivariate function, in the form of a vector which contains the partial derivatives with respect to each variable. We use the directional derivative to show that the gradient points in the direction of steepest ascent.

Thanks for your time and attention. This was a bit of a long post, but I hope that you were able to find it useful. Comments and feedback is more than welcome. In the next section, we will survey and contrast the three Gradient Descent variants: Batch, Stochastic, and Minibatch Gradient Descent.

Cheers,

J2