<script type="text/x-mathjax-config">
MathJax.Hub.Config({
TeX: {
equationNumbers: {
autoNumber: "all",
formatNumber: function (n) {return ''+n}
}
}});</script>

![](../content/figures/top_view.jpg)

# What is Gradient Descent?

> Gradient descent is a first-order iterative optimization algorithm for finding the minimum of a real-valued, differentiable objective function: $\mathbb{R}^n \to \mathbb{R}$. Parameterized by $\theta \in \mathbb{R}^n$, Gradient Descent minimizes objective functions by iteratively updating model parameters in the direction opposite to the gradient of the objective function $\nabla_\theta J(\theta)$.

Wait! What? If that definition works for you, skip ahead to the next section. Otherwise, if you are like me (and I know I am), perhaps an English language translation may be useful.

## Steepest Descent Challenge
Consider this. It's late May, daybreak at 58° 54′ 23″ N, 137° 31′ 36″ W. A dense fog descends over the summit of Mount Fairweather (no, really), British Columbia, bringing into stark relief, your team's Brobdingnagian challenge ahead. The six-day ascent, characterized by "fair weather" and spectacular views, would be a mere warm-up for the descent.

The goal? Descend Mount Fairweather to base camp using the path of steepest descent. 
1. 



characterized the five-day climb to the summit. The five-day climb to the summit was completed 
the crystallizing the team's placing in stark relief,  a cloud over the team's celebratory mood. Clearly, the descent would be as hairy as the 5 day climb to  




The team's celebratory mood gave way to slight apprehension as an intense fog descends.
The celebratory mood 
MountThe team's celeb celebrates another successful summit in the usual way, a cup of store-brand tea with a Snickers bar. The The celebratory You and your team are sharing cups of tea to celebrate a successful summit up the face of Mount Fairweather in British Columbia when the mother of all fog patterns descends. You brush back the snow from the face of your Garmin Fenix. The elevation reads 3732 m. Time? 8:15 am. Your goal? Get you and the team safely down to base camp at Paradise Valley by twilight, uncork that bottle of Macallan Fine Oak 30 you had been saving, and toast the team's triumphant ascent and successful Gradient Descent.

Based upon the elevation, you are able to estimate your current location on the the map to within a couple hundred meters. The plan? Follow the path of steepest descent down the face to Paradise Valley Lodge. The team has to descend about 2400 meters over a horizontal distance of approximately 22 kilometers. According to Naismith’s rule, it should take the team about  11 hours to complete the descent. Plenty of water and food and about 8 hours of daylight remaining. You wouldn't be anywhere else, but here.

You assumed your position at the front of the file. With about ten meter visibility, you arc your Lacroix LX Telescopic ski pole 180 degrees in the downhill direction. Then you take 10 steps in the direction of steepest descent, and repeat. Yeah, it was going to be a slow slog through the persistent fog. 

Early progress was hindered by high variance in the path of descent. Limited visibility forced you to take elevation measurements and to adjust your path of steepest descent every 10 steps. This effective batch size of 10 steps lead to a highly stochastic and noisy descent. That said, the average gradient over the second leg were among the highest gradients observed during the descent ($\nabla \approx 0.268$). This introduced a slight dampening effect on the noise introduced by the small batch size as we made our way to the first waypoint. Over the first leg, the team descended some 1000 meters over a distance of Euclidean distance of 3.86 kilometers for a rate of 48 minutes per kilometer. The Garmin showed 11:21 am and an elevation 2432 meters. With the first leg complete, you guzzled some water, choaked down a powerbar, then prepared yourself for the second leg. Eighteen kilometers to go.

Morale was high for the second leg, despite diminishing visibility. As a consequence of increased fog density, batch size was further reduced from 10 steps to 5 steps per measurement. In addition, the average angle of descent fell to 10 degrees from 15 degrees, reducing the gradient and the associated step size by 34%. Yet the pace  improved 22% from 48 to 37.5 minutes per kilometer, as the team descending some 500 meters over a Euclidean distance of 2.9 kilometers. By 1:09 pm, the team had descended some 1500 meters over a distance of 6.74 kilometers. The second leg was complete with about 15 kilometers to go. Estimated time of arrival, 7:00 pm.


A map reconnaissance of the third leg revealed a steepest descent path which passed perilously close to Fairweather Glacier, an alternate camp at an estimated elevation of 800 meters. Sailing off course towards Fairweather Glacier would mean another night in the wild, no scotch and a pis&-ed off climbing team. Fortunately, some of the fog has lifted, and the increased visibility would allow the team to return the batch size back to 10 steps, from five, thereby reducing variance in the descent path. On the other hand, the angle of descent would drop from 10 to 5 degrees, resulting in a 80% reduction in the gradient to a magnitude $\approx 0.035$. Diminishing gradients are difficult to distinguish in when using steepest descent and, as a consequence, error may be propagated into the descent path. Countermeasures must be taken to ensure that the descent path does not converge at Fairweather Glacier. The third leg would cover some 5.7 kilometers and a vertical distance of 200 meters. Transit time was 1.9 hours or 20 minutes per kilometer. The third leg was completed at 3:03 pm with 9.2 kilometers remaining. Estimated time of arrival, 7:15pm

The forth leg began with some apprehension. Given the compounding effects of navigation errors, we could not discount the likelihood of being off course, or converging towards Fairweather Glacier. A map reconnaissance to confirm our current position and trajectory was inconclusive at a precision of 50 meters. The forth leg would descend another 100 meters over a distance of 5.7 kilometers. The average gradient would be $\approx$ 0.0174, corresponding to an approximate average angle of descent of one degree. Despite increased visibility, the path of steepest descent would be exceedingly difficult to discern with precision and would be prone to noise. We had three options. 

**Option 1: SGD with Learning Rate Decay**: Run SGD with a starting learning rate $\alpha=1$, then decay it by a factor of $2$ each 50 steps for 11 cycles or 550 steps and a ending learning rate of $\alpha=0.000976$. In the early cycles, one should observe large step sizes with high variance. This should obviate settlement into the suboptimal minimum at Fairweather Glacier. As the learning rate anneals, the step sizes and the variance in the trajectory should decrease as you approach leg length of $\approx 6 kilometers.
**Option 2: SGD with Increased Batch Size**: Run SGD with a constant learning rate $\alpha=0.01$ and a starting batch size of $2$. Increase the batch size by a factor of $2$ each 50 steps for 11 cycles. Maximum batch size of $2048$ will be encountered at 500 steps. In the early phase of training this leg, we should see high variance, smaller batch sizes and more frequent updates. As the batch size increases, parameter updates and variance falls as gradient estimates improve as you approach the leg length of 6 kilometers.  
**Option 3: SGD Hybrid**: Start with learning rate $\alpha=1$ and batch size = $2$. For the next 5 cycles, increase batch size by a factor of $2$ eac 50 steps, until you reach a maximum batch size of 64. After that, the batch size remains constant and the learning rate decays by a factor of $2$ each 50 steps, until a minimum learning rate $alpha=0.03125$ is observed. In the early stages of training, we should observe large step sizes with high variance. Variance should gradually fall as batch size increases to 64. Then, step sizes should gradually decrease along with variance until a minimum learning rate of $\alpha=0.03125$ is observed.


over  that has propagated try to   undermined As you check the maps, you As you guzzle down water, you notice less chatter amongst the team and you can't help wondering  It was just after 3:00pm and We made waypoint #3 at around 3pm. This last leg covered a vertical descent of just 200 meters over a horizontal  distance of 5.7 kilometers. By this time, we had traversed a total of 12.5 kilometers, over 5.7 kilometers since waypoint $2choosing the direction of steepest descent became so difficult that we could not be certain that we were on course.  difficult to determine the direction of the next step. Visibility had not improved and you wondered if you had arrived at Fairweather Glacier. The map check was inconclusive. You chose to trust your navigation and carefully monitor the elevation and continue until you reach Paradise Valley at 209m or you observed a kilometer of continuous climbing. 

Fifty five minutes of daylight remaining. Temperature dropped to -20 C. Circulation in your extremities affected. Down to last 50 grams of Gatorade when you notice a three degree change in elevation.  Five degrees, seven degrees, then 10 degrees of descent. You have traversed a saddle point in the terrain and you were now on the final descent into Paradise Valley. Step sizes increased along with the angle of descent. The barely perceptible sounds of voices and laughter became increasingly vivid with each step. The fog began to clear leaving the blue neon sign, 'Paradise Valley' in stark relief. 'Welcome to Paradise Valley Lodge, what can I get for ya, she said in that disarming Canadian accent? Oban...neat.

While taking a sip of gaterade, it occurs to you that Four hours of daylight.    steps are proportional to the angle of descent Following the path of steepest descent, you make early average hour for 4000 meters flat distance, plus  You are able to make reasonably good progress early  onyou determine the direction of steepest descent and take a step in that direction. down the mountain face and take a step   you You take a few sips of gaterade as you check your trail map




Suppose you have found yourself hiking alone in the mountains of British Columbia one late January afternoon as a dense fog rolls in. The trail map from your hotel's concierge desk is practically useless as visibility has been reduced to about 5 feet. Two hours of daylight, a half bottle of gatorade, and a couple of powerbars stand between you and a rather cool oblivion. Your goal? The valley basin ski lodge cocktail lounge, with the greatest dispatch.

You take a small bite of the powerbar, a sip of the gaterade and take note of your elevation.  Which way is down?  Using your Leki Stealth S Ski Pole, you survey the ground to determine which direction reduces your elevation the most and take a single step, a big step in that direction. Again, you test for the direction of steepest descent. One hour and fifty five minutes of daylight and a 360 degree white-out. Take a step in the direction of steepest descent. Repeat.

As you maneauver down the face towards the valley, you can't help but wonder if continuing in this fashion will lead you back to the valley basin or will it lead to another resort.  back to the ski lodge or some other  Take a sip of the gatorade and a step in the direction You take a step in the direction of steepest descent as you take in the 360 degree white-out? Which way is down? Visibility is down to two feet. One hour and fifty-five minutes of daylight. Testing Using your Leki Stealth S Ski Pole yo

lGet off this damn mountain and back to your hotel in 

Your goal? Make it back to your hotel in the valley basin before you freeze 




There are three variants to Gradient Descent and they are:    

* Batch Gradient Descent    
* Stochastic Gradient Descent     
* Mini-batch Gradient Descent      

They are distinguished by the amount of data that is used to compute the gradient of the objective function, which affects the accuracy of the parameter updates, and the time complexity of the algorithm.

## Batch Gradient Descent
Batch Gradient Descent (BGD) computes the gradient of the objective function with respect to the parameters $\theta$ using the *entire* training set in a single batch.  

```{python intro_bgd, echo=TRUE, eval=FALSE}
theta = np.random.rand(data.shape[1])
for i in range(epochs):
  gradient = compute_gradient(loss_function, data, theta)
  theta = theta - learning_rate * gradient
```

First, we randomly initialize the parameters $\theta$. Then, for a designated number of epochs, we compute the gradient of the loss function, for the entire data set, with respect to the parameters $\theta$. Next, we update the parameters $\theta$ in the direction of the negative gradient, scaled by the learning rate.

### Pros
* Batch Gradient Descent is guaranteed to converge to a global minimum for convex error surfaces and to a local minimum if the error manifold is non-convex.
* Using the entire dataset yields unbiased, more stable estimates of the true gradients.   
* By averaging the gradient over all examples, the optimization path has fewer oscillations and less noise as it converges towards its minimum.    
* The gradient computation can be vectorized, allowing for parallelized computation and exploitation of GPU architectures. 

### Cons
* Despite vectorization, computing the gradient is very expensive because it evaluates the model on every example in the entire data set.          
* Each update requires every example, even though some examples may be redundant, contributing little to the gradient.       
* Renders a stable error gradient which is unable to escape saddle points and sub-optimal local minima for non-convex error surfaces.   
* The entire training set must be memory resident, which becomes intractable for very large training sets.    

## Stochastic Gradient Descent
Stochastic Gradient Descent (SGD), in contrast to batch Gradient Descent, computes the parameter update for *each* training example.

```{python intro_sgd, echo=TRUE, eval=FALSE}
theta = np.random.rand(data.shape[1])
for i in range(epochs):
  np.random.shuffle(data)
  for example in data:
    gradient = compute_gradient(loss_function, example, theta)
    theta = theta - learning_rate * gradient
```
The SGD algorithm differs from batch Gradient Descent in two respects. First, we shuffle the data at the beginning of each epoch to avoid cycles in the learning process. Second, the gradient computation and parameter updates occur within an inner loop that iterates over each example.

### Pros
* Greater computation and memory efficiency since each update is based upon just a single example. This computational efficiency is often leveraged by performing many more iterations of SGD than would be performed using batch Gradient Descent.
* Large datasets tend to train faster with SGD than with BGD, due to higher frequency parameter updates.     
* For non-convex error manifolds, SGD tends to produce better results than those of BGD. The SGD optimization path is characterized by greater noise and volatility. This additional energy can enable Gradient Descent to escape saddle points and suboptimal local minima.    

### Cons   
* Processing a single example at a time loses the computational advantages of vectorized and parallelized operations.    
* The high variance in the gradient computations complicates convergence as it causes SGD to overshoot and oscillate around the exact minimum. However, it has been shown that slowly decreasing the learning rate enables SGD to achieve the same convergence behaviour as batch Gradient Descent, converging to local or the global minimum for non-convex and convex optimization respectively [@Ruder2016]. 

## Mini-batch Gradient Descent
Mini-batch Gradient Descent stakes out the goldilocks zone between batch Gradient Descent and SGD by performing the parameter updates after each mini-batch of $n$ training examples. 

```{python intro_mbgd, echo=TRUE, eval=FALSE}
theta = np.random.rand(data.shape[1])
for i in range(epochs):
  np.random.shuffle(data)
  for batch in get_batches(data, batch_size=32):
    gradient = compute_gradient(loss_function, batch, theta)
    theta = theta - learning_rate * gradient
```

As with Stochastic Gradient Descent, we shuffle the data, then iterate through a batch generator. This generator  yields batches of $m^\prime$ training examples, where $m^\prime$ typically ranges from about 32 to a few hundred in powers of two. For each batch, we compute the gradient of the loss function with respect to the parameters $\theta$, then perform the updates in accordance with the negative gradient and the learning rate. 

### Pros
* With smaller batch sizes, mini-batch Gradient Descent can make problems involving large datasets more computationally tractable than they would otherwise be with batch Gradient Descent.   
* Mini-batch Gradient Descent can produce gradient estimates as good as (or no worse than) the gradient estimates from batch Gradient Descent. The full training set is likely to include noise, outliers, and redundant samples. Recall that the training set is an *approximation* of a true data generating distribution and is likely to include noise, outliers, and redundant samples. Any randomly sampled mini-batch may reflect the *true* data generating distribution as effectively as the full training set. Averaging over several iterations of the mini-batch Gradient Descent can produce better gradient estimates than those computed from a full batch.     
* Mini-batch gradients combine the stability of batch Gradient Descent with the noise of stochastic Gradient Descent. As such, it produces a lower-variance gradient with enough noise to escape local minima of non-convex error surfaces.  
* Mini-batch Gradient Descent can produce models with acceptable generalization error faster than batch and stochastic Gradient Descent. Larger batch sizes (>1) reduce the variance of stochastic updates by taking the average of the gradient over the mini-batch. This allows one to take larger steps towards the optimum. The computational benefit comes from the fact that a large mini-batch is trivial to parallelized on a GPU, multiple GPUs or multiple machines.  With smaller batch sizes (<m), mini-batch Gradient Descent allows one to iterate over the error surface faster than with batch Gradient Descent. As stated above, average gradients over a mini-batch oftentimes approximate the true data generating distribution as well as batch gradient does.

### Cons    
* Mini-batch Gradient Descent requires careful tuning of batch size and learning rate. Large batch sizes allows one to use larger learning rates, up to an algorithmic, problem specific upper bound which depends on the smoothness of the objective function. (The theoretical limit is typically 1/L, where L is the Lipschitz constant of the "full" gradient). 
* Mini-batch Gradient Descent applies the same learning rate to every gradient update. As the gradient approaches its optimum, smaller learning rates are often needed. In such situations, one may consider increasing the batch size rather than reducing the learning rate for improved parallelism.  

## Key-Take-aways
1. The three variants, Batch Gradient Descent, Stochastic Gradient Descent and Mini-batch Gradient Descent differ largely in the amount of data used to compute the gradients and to perform the gradient updates.  
