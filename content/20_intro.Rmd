<script type="text/x-mathjax-config">
MathJax.Hub.Config({
TeX: {
equationNumbers: {
autoNumber: "all",
formatNumber: function (n) {return ''+n}
}
}});</script>

```{python 20_intro, code=readLines('./code/20_intro.py'), results='hide'}
```

# Introduction to Gradient Descent 

So, what is gradient descent?

> Gradient descent is a first-order iterative optimization algorithm for finding the minimum of a real-valued, differentiable objective function: $\mathbb{R}^n \to \mathbb{R}$. Parameterized by $\theta \in \mathbb{R}^n$, gradient descent iteratively updates the parameters in the direction opposite to the gradient of the objective function $\nabla_\theta J(\theta)$.

If that definition works for you, skip ahead to the next section.  Otherwise, if you are like me (and I know I am), hang with me. Let's start by modeling a simple linear regression example.

## Linear Regression Model 
Consider the task of predicting home prices based upon data provided in a training set consisting of set of features $X$, including for instance:    

* per capita crime rate (crim)     
* proportion of residential land zoned for lots over 25,000 sq.ft, (zn)     
* proportion of non-retail business acres per town (indus)    
* average number of rooms per dwelling (rm)   
* weighted mean of distances to five Boston employment centres (dis)    
* and others,

and the target variable $Y$, the sales price of the home in $1000s. For visualization purposes and to keep things simple, we'll limit ourselves to a single predictor, **rm**, average number of rooms per dwelling. Hereinafter, we will refer to this single predictor as **rooms**.

The scatterplot below shows the relationship between the number of rooms and the home price in the data set.
![](./figures/price_by_rooms.png)

`r kfigr::figr(label = "price_by_rooms", prefix = TRUE, link = TRUE, type="Figure")`: Housing Prices by No. Rooms

As one my expect, prices appear to be positively associated with the number of rooms.

Our task is to learn a function, a *hypothesis* function, that renders a sales price prediction given the number of rooms in the home. Our hypothesis function is given by:    

$$h_\theta(x)=\theta_0+\theta_1x_1\label{eq:hypothesis}$$
where:       

* $\theta_0$ is bias term    
* $\theta_1$ is the coefficient for $x_1$, rooms       
* $h_\theta(x)$ is our hypotheses or home price predictions

This is a good time to define some notation that we will use later.
```{r notation_sl, results='asis'}
tbl <- "
|     Notation     |  Description |
|------------------|:-----------------------------------------------------------------------|
| $X$ | The set of training examples |
| $y$ | The set of labeled target values for the training examples |
| $n$ | The number of variables in X |
| $m$ | The number of observations in X |
| $(x^{(i)},y^{(i)})$ | The $i$-th example pair in X (supervised learning) |
| $\\mathbb{R}$ | The set of real numbers. |
| $X \\in \\mathbb{R}^{n\\times m}$ | Design matrix, where $X_{i,:}$ denotes $x^{(i)}$ |
| $\\theta$ | A vector of parameters in $\\mathbb{R^n}$ |
| $h(x)$ | A hypothesis function |
| $\\hat{y}$ | Label predicted by a function $h$ |
"
cat(tbl)
```

So the $\theta$'s are the parameters, or weights that parameterize the space of linear functions mapping $X\to Y$. Note, that we have two parameters to learn, $\theta_0$, the bias term and $\theta_1$, the coefficient for rooms. To simplify our notation, and to make our hypothesis function suitable for matrix multiplication, we will add a column of ones to $X$, so that $x_0=1$ (this is the **intercept term**). We therefore have the following simplified form
$$h_\theta(x)=\displaystyle\sum_{i=0}^n\theta_ix_i=\theta^Tx,$$
where $\theta$ and $x$ are both vectors $\in\mathbb{R}^n$ and $n$ is the number of input variables (not counting $x_0$. Our goal, therefore, is to learn the parameters $\theta$ that make $h(x)$ as close to $y$ as possible for all $x$'s and $y$'s. So, how do we do that?  

Well we need some way of measuring how good a prediction actually is. Enter the cost function.

## Cost (Objective) Function
In order to learn the parameters $\theta$ that that make $h(x)$ as close to $y$ as possible, we need a function that measures, for each value of the $\theta$'s, how close $h(x^{(i)})$'s are to the corresponding $y^{(i)}$'s. A *loss* function for the $i$th sample quantifies a *penalty* for predicting $\hat{y_i}$ when the true value is $y_i$, and is given by:
$$L(x^{(i)},y^{(i)};\theta)$$
There are several loss functions used in regression and classification problems, but the **quadratic** loss function is most commonly used for regression tasks and is given by:
$$L(x^{(i)},y^{(i)},\theta)=\frac{1}{2}(\theta^Tx^{(i)}-y^{(i)})^2,$$

Note that this loss function $L$ measures the quality of the prediction for an *individual observation*. If the prediction is perfectly accurate, the loss function is zero. If not, the loss function measures “how bad” the prediction is. 

To measure the quality of the predictions for a full training set $X$, we take the average of the loss $L$ across all observations. Concretely, this **cost** function is given by:

$$J(\theta)=\frac{1}{2m}\displaystyle\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2\label{eq:cost}$$

`r kfigr::figr(label = "convex_cost", prefix = TRUE, link = TRUE, type="Figure")` provides a 3d visualization of the cost surface for the quadratic loss function. The values of our parameters are plotted horizontally, and their associated costs are rendered in the vertical direction. The quadratic cost function is characterized by its convex shape and the existence of a single *global* minimum.  

![](./figures/quadratic_cost_surface.png)

`r kfigr::figr(label = "convex_cost", prefix = TRUE, link = TRUE, type="Figure")` Cost Surface for Quadratic Cost Function 

The parameter values associated with the minimum cost are said to *minimize* the cost function.  We can therefore redefine our prediction task. Our goal now is to find the values for the parameters $\theta$ that minimize the following cost function. 

$$\text{min.}\space J(\theta)=\frac{1}{2m}\displaystyle\sum_{i=1}^m(\theta^Tx^{(i)}-y^{(i)})^2.$$
Note, the cost function is also referred to as an **objective function** in optimization contexts. I will be using both terms interchangeably.

## The Gradient
Now that we have our objective function, we seek the model parameters $\theta$ that minimize $J(\theta)$. Referring to `r kfigr::figr(label = "steepest", prefix = TRUE, link = TRUE, type="Figure")`, let's assume we've randomly initialized our parameters $\theta_0$ and $\theta_1$. Applying our cost function $\ref{eq:cost}$, would place us at the start point indicated below.

![](./figures/convex_cost_surface_annotated.png)

`r kfigr::figr(label = "steepest", prefix = TRUE, link = TRUE, type="Figure")`: Gradient $\in \mathbb{R}^2$

Next, we need to determine, out of all possible directions in the $x-y$ plane, which direction would bring about the greatest reduction in the value of the cost function. This direction is given by the negative of the **gradient** of the objective function. The multivariate analog of the derivative, the gradient is a vector valued function where the components are partial derivatives of the cost function $J(\theta)$ with respect to each parameter $\theta$.
The gradient for our *quadratic* cost function $\ref{eq:cost}$ is given by:
$$\nabla J(\theta)=\frac{1}{m}\displaystyle\sum_{i=1}^m(\theta^Tx^{(i)}-y^{(i)}) \cdot x_j^{(i)}\space\text{(for every j).}$$
where:    

* $\nabla$ is the notation for the gradient,       
* $m$ is the number of observations,     
* $(i)$ denotes the $i$th observation,     
* $x^{(i)}$ are the features for the $i$th observation, the intercept term and rooms,           
* $y^{(i)}$ is the sales price for the $i$th observation,      
* $x_j^{(i)}$ is the $j$th feature of $x^{(i)}$  
* $\theta$ is the parameter vector   

As shown in `r kfigr::figr(label = "steepest", prefix = TRUE, link = TRUE, type="Figure")`, the gradient also happens to be the direction of steepest *ascent* of the cost function. Since we are minimizing $J(\theta)$, we want to move in the direction *opposite* the gradient, the direction of **steepest descent**. Ergo the algorithm's name.

## Learning Rate
Now that we have the direction, we need to specify our step size. This is given by the **learning rate** $\alpha$. 

![](./figures/learningrates.jpeg)

Image by [Andrej Karpathy](http://cs231n.github.io/neural-networks-3/)

`r kfigr::figr(label = "learning_rates", prefix = TRUE, link = TRUE, type="Figure")`: Learning Rates

The learning rate must be carefully chosen, because if it is too small (blue line) the algorithm will take an excessive amount of time to train. High learning rates (green line) decay the loss faster, but they may cause the algorithm to oscillate around a minimum, resulting in higher loss values. Excessively high learning rates (yellow line) may cause the algorithm to diverge from the minimum all together. 

## Parameter Update
Given the gradient $\nabla J(\theta)$, and a suitable learning rate $\alpha$, we are now ready to update our parameters $\theta$ in order to 'move' to the new location which minimizes the cost function the most. We do this by performing the following update to $\theta$:

$$\theta_j=: \theta_j - \alpha \nabla J(\theta_j)\space\text{(for every j).}$$


We simultaneously subtract the gradient multiplied by the learning rate from the current parameters $\theta_j$ to move to the new location in the $x-y$ plane. Again, all updates to the $\theta$'s are confined to the $x-y$ plane.  Therefore, the updates are projections of the components of the gradient vector onto the $x-y$ plane as shown in `r kfigr::figr(label = "steepest", prefix = TRUE, link = TRUE, type="Figure")`.

Once the parameters are updated, the gradient is computed for the new location and the process repeats until a stopping condition is met or convergence is achieved.

## Gradient Descent Algorithm
The following pseudo-code summarizes the gradient descent iterative algorithm.

Parameters: Training Set: $\tau$; learning Rate: $\alpha$; Normal Distribution Std: $\sigma$; epochs
1. Initialize parameters $\theta$ with random normal distribution $\theta \sim N(0,\sigma^2)$
2. for i in range(epochs)
3.    gradient = compute_gradiant(loss_function, $\tau$, $\theta$)
4.    $\theta$ = $\theta$ - $\alpha$ * gradient
5. return $\theta$

We begin by initializing the parameters $\theta$ with the random normal distribution. Next, we iteratively compute the gradient, then perform the update to the parameters $\theta$, for some predefined number of epochs.  

Let's see it in action.

## Gradient Descent in Action
For this demonstration, we used the [Boston Housing housing](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html#sklearn.datasets.load_boston) dataset. For visualization purpose, we modeled a single predictor, the mean number of rooms. Three models were fit with learning rates 0.001, 0.01, and 0.1 respectively. Each model was trained for 500 epochs. Here are the gradient descent trajectories.
![](./figures/search_by_learning_rate.gif)

`r kfigr::figr(label = "search_by_learning_rate", prefix = TRUE, link = TRUE, type="Figure")`: Gradient Descent Trajectories for Various Learning Rates

The convergence rates for the selected learning rates are evident. Let's see how the models fit the data.

![](./figures/fit_by_learning_rate.gif)

`r kfigr::figr(label = "fit_by_learning_rate", prefix = TRUE, link = TRUE, type="Figure")`: Model Fit for Various Learning Rates

Viola! 

## Key Takeaways
1. Gradient descent is a first-order iterative optimization algorithm for finding the minimum of a real-valued, differentiable objective function: $\mathbb{R}^n \to \mathbb{R}$. Parameterized by $\theta \in \mathbb{R}^n$, gradient descent iteratively updates the parameters in the direction opposite to the gradient of the objective function $\nabla_\theta J(\theta)$.

2. The (linear regression) model representation begins with a hypothesis function $h_\theta(x)$ which renders a prediction based upon a set of features $X$ and a set of parameters or weights $\theta$ and is given by:

$$h_\theta(x)=\displaystyle\sum_{i=0}^n\theta_ix_i=\theta^Tx,$$
3. The **loss function** renders a penalty for predicting $\hat{y}$ when the true value is $y$. We denote the loss function for the $ith$ observation as:
$$L(x^{(i)},y^{(i)};\theta)$$
There are a variety of loss functions that can be used in regression and classification settings. The quadratic loss function is the most commonly used in regression settings and is given by:  
$$L(x^{(i)},y^{(i)},\theta)=\frac{1}{2}(\theta^Tx^{(i)}-y^{(i)})^2$$
4. A loss function computes the penalty for a single observation. A **cost function**; on the other hand, renders the penalty for a set of observations: either a batch or an entire training set.  The quadratic cost function is the average of the per-observation losses and is given by:

$$J(\theta)=\frac{1}{2m}\displaystyle\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2$$
5. Now that we've measured the cost associated with a set of parameters $\theta$, we minimize the cost function by taking a set in the direction of steepest descent, represented by a vector of partial derivatives with respect to the parameters $\theta$. This vectored valued function is called the **gradient** and is given by:

$$\nabla J(\theta)=\frac{1}{m}\displaystyle\sum_{i=1}^m(\theta^Tx^{(i)}-y^{(i)}) \cdot x_j^{(i)}\space\text{(for every j).}$$
6. Once the gradient is computed, we update the parameters $\theta$ by subtracting the gradient vector, scaled by the learning rate $\alpha$ as follows:

$$\theta_j=: \theta_j - \alpha \nabla J(\theta_j)\space\text{(for every j).}$$
7. Finally, we repeat 1-6 for a predetermined number of epochs or until a convergence criterion is met. 

## Next Section
In the next section, we will develop the mathematical intuition behind gradient descent.