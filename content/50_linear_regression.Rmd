<script type="text/x-mathjax-config">
MathJax.Hub.Config({
TeX: {
equationNumbers: {
autoNumber: "all",
formatNumber: function (n) {return ''+n}
}
}});</script>

# Linear Regression with Gradient Descent
This is section __ of a 6-part series on Gradient Descent. In the prior section, we examined the calculus behind Gradient Descent. Here, we delve into model representation, cost function and gradient for linear regression in order to:

* understand the motivation and assumptions behind the linear regression hypothesis space     
* derive the quadratic Mean Squared Error (MSE) cost function    
* justify the cost function in terms of maximum likelihood estimation         
* define the gradient of the MSE cost function    

Linear regression, the most widely used of all statistical techniques, is the study of linear, additive relationships between variables. Given:

* a set of $m$ training examples with independent variables $X=\{x^{(1)},x^{(2)},\dots,x^{(m)}\}$, and dependent variable $Y=\{y^{(1)},y^{(2)},\dots,y^{(m)}\}$, 
* where each example contains a feature vector $x\in \mathbb{R}^N$ real-valued attributes $x^{(i)T}=\{x^{(i)}_1,x^{(i)}_2,\dots, x^{(i)}_n \}$.   

our task is to find a hypothesis function $h(x)$ that best approximates the true target function $f(x)$ such that $\forall x, h(x) \approx f(x)$.

## Linear Regression Assumptions
We suppose that the true data generating model is:
$$Y=\theta_0+\displaystyle\sum_{i=1}^n\theta_ix_i+\epsilon$$
where:    

* $\theta_0$ is the bias term     
* $\theta_i$ is the $i$th coefficient, or parameter    
* $x_i$ is the $i$th variable in $x$    
* $\epsilon$ is the error term         
* $n$ is the number of variables or features in $x$    

Four principal assumptions will shape the hypothesis space, the space of candidate functions $h(x)$.

**1. Linearity**:The central property of this function is that the expected value of the dependent variable $Y$ is a linear function of each of the independent variables, holding the others constant.      
**2. Additivity**: The effects of the different independent variables on the expected value of the dependent variable are additive.     
**3. Errors are I.I.D.**: Errors $\epsilon$ are independently and identically normally distributed such that $\epsilon \sim \mathcal{N}(0,\sigma^2)$      
**4. Homoscedasticity**: Errors $\epsilon$ have a constant variance vis-a-vis predictions and independent variables.    

## Linear Regression Hypothesis Space
Given the assumptions above, our hypothesis space is the set of linear functions of the form:
$$h_\theta(x)=\theta_0+\displaystyle\sum_{i=1}^n\theta_ix_i$$
To simplify our notation, we can add an intercept term $x_0=1$ to our feature vector $x$. We can now represent our hypothesis in the *vectorized* form:
$$h_\theta(x)=\displaystyle\sum_{i=0}^n\theta_ix_i=\theta^Tx$$
The variables $X$ can come from different sources:  

* quantitative inputs,     
* transformations of quantitative inputs, such as log, square-root or square,      
* basis expansions, such as $X_2=X_1^2, X_3=X_1^3$ leading to a polynomial representation,     
* numeric or "dummy" coding of levels of quantitative inputs,     
* interactions between variables, such as $X_3=X_1\times X_2$

Regardless of the source, the hypothesis space is linear in the parameters  [@hastie01statisticallearning]. We assume a linear relationship between the independent and dependent variables because:    

* linear relationships are the simplest *non-trivial* relationships that one can imagine,    
* linear relationships are the easiest to work with,    
* the 'true' relationships between our variables are often at least *approximately* linear over the range of values of interest,       
* non-linear relationships can be linearized by transforming the variables    

Each parameter $\theta_1, \theta_2, \dots, \theta_n$ is a real number, and is associated with one of the input features $x_1, x_2, \dots, x_n$. The parameter $\theta_j$ represents how important that input $x_j$ is to the hypothesis or prediction. The additional constant $\theta_0$, the bias term, is the prediction that the model would make if all the $x$'s were zero.

## Linear Regression Cost Function
Having defined our hypothesis space, our next step is to specify a *cost* function to measure the quality of our model. The overall cost of predicting $\hat{y}$ on the training set when the true values are $y$ is given by:
$$J(\theta)=\frac{1}{2m}\displaystyle\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2$$
This **quadratic** function, also known as the least squares or Mean Squared Error (MSE) cost function, is convex with a single global minimum, well suited for Gradient Descent optimization.

There are several other cost functions used in regression settings, including Mean Squared Logarithmic Error Loss and Mean Absolute Error Loss, but the quadratic, Mean Squared Error (MSE) cost function is essentially the default for regression problems.  Why? It has to do with **maximum likelihood estimation (MLE)**. 

### Maximum Likelihood Estimation (MLE)
MLE is an approach for estimating the parameter or parameters $\theta$ of a model. Given a dependent variable $Y=Y_1, Y_2,\dots, Y_m$, a feature matrix $X_1, X_2,\dots, X_3$ and parameters $\theta$ and $\sigma^2$, MLE measures the propensity that the data was generated from the model. The parameter estimate, obtained by maximizing a **likelihood function** is called the maximum likelihood estimate $\hat{\theta}_{MLE}$ 

#### Likelihood Function
Recall that our statistical model is defined as follows:    

* The distribution of $X$ is arbitrary and perhaps non-random.     
* $y=\theta_0+\displaystyle\sum_{i=1}^n\theta_ix_i+\epsilon$    
* $\hat{y}=\theta_0+\displaystyle\sum_{i=1}^n\theta_ix_i$    
* Adding the bias term to the feature matrix $X$, $\hat{y}=\theta^Tx$
* $\epsilon \sim mathcal{N}(0,\sigma^2)$, and is independent of X.      
* $\epsilon$ is independent across observations.    

Consequently $Y$ is independent across observations and $Y$ is normally distributed around $\hat{Y}$ as such, $y\sim \mathcal{N}(\hat{y},\sigma^2)$. Because of these strong assumptions, we can express the conditional probability of observing the output $y$, given $x$ and the model parameters $\theta$ and $\sigma^2$ as the Guassian Probability Density Function (PDF):  
$$p(y|x;\theta,\sigma^2)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(y-\mu)^2}{2\sigma^2}}$$
Since we don't know the true population parameters, we substitute:    

* $\mu$ with $\hat{y}=\theta^Tx$   
* $\sigma^2$ with $s^2$, the sample variance

This yields the probability density function of $Y$ for each $x$ as:
$$p(y|x;\theta,s^2)=\frac{1}{\sqrt{2\pi s^2}}e^{-\frac{(y-\theta^Tx)^2}{2s^2}}$$
The Guassian probability density functions with various parameterizations are plotted below. The red distribution is the standard Gaussian $P(x) \sim \mathcal{N}(0,1)$.
![](./figures/pdf.png)

`r kfigr::figr(label = "sigmoid", prefix = TRUE, link = TRUE, type="Figure")`: A selection of Gaussian Probability Density Functions (PDFs) **Source**: Wikimedia Commons

The **likelihood function** for the training set is the product of the individual probability densities.
$$\mathcal{L}(\theta,s^2)=\prod_{i=1}^mp(y^{(i)}|x^{(i)};\theta,s^2)=\prod_{i=1}^m\frac{1}{\sqrt{2\pi s^2}}e^{-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2s^2}}\label{}$$
Probability densities are relatively small numbers and multiplying them may lead to arithmetic underflow. So its often easier to work with the natural logarithm of the likelihood function, which we will call the **log-likelihood**. Taking the log of equation ($\ref{likelihood}$) and applying the product rule, the log-likelihood becomes:

$$
\begin{align}
\text{log }(\mathcal{L}(\theta, s^2))&=\text{log }\prod_{i=1}^mp(y^{(i)}|x^{(i)};\theta,s^2)\\
&=\displaystyle\sum_{i=1}^m\text{log }p(y^{(i)}|x^{(i)};\theta,s^2)\\
&=-\frac{m}{2}\text{log }2\pi-m\text{ log }s-\frac{1}{2s^2}\displaystyle\sum_{i=1}^m(y^{(i)}-(\theta^Tx^{(i)}))^2
\end{align}
$$
Since the logarithm is a monotonically increasing function, that is to say that if $x_1>x_2$, then $\text{log}(x_1)>\text{log}(x_2)$, the value that maximizes the likelihood is also the value that maximizes the log-likelihood function. 

To find the value of $\theta$ that maximizes the log-likelihood function, we find its critical point, the point at which the function's partial derivative with respect to parameters $\theta$ is zero:
$$\frac{\partial\mathcal{L}(\theta)}{\partial \theta}=0$$
We can remove all the terms that do not contain our parameter $\theta$ as they do not have any effect on the optimization problem. Hence, our objective function can be reduced to:
$$
\begin{align}
\text{log}(\mathcal{L}(\theta, s^2))&=-\displaystyle\sum_{i=1}^m{(y-\theta^Tx^{(i)})^2}
\end{align}
\label{ref:ll}
$$
Therefore, the value $\theta$ that maximizes equation ($\ref{ll}$) is the maximum likelihood estimate $\hat{\theta}_{MSE}$:
$$\hat{\theta}_{MSE}=\text{arg max}\bigg[-\displaystyle\sum_{i=1}^m(y^{(i)}-\hat{y}^{(i)})^2\bigg]\label{ref:mse}$$
Maximizing a function is the same as minimizing its negative, so we can rewrite (${\ref{mse}}$) as:
$$\hat{\theta}_{MSE}=\text{arg min}\displaystyle\sum_{i=1}^m(y^{(i)}-\hat{y}^{(i)})^2$$
Taking the average across all observations, we get:
$$\hat{\theta}_{MSE}=\text{arg min}\frac{1}{m}\displaystyle\sum_{i=1}^m(y^{(i)}-\hat{y}^{(i)})^2$$
Viola! Aside from the 2 in the denominator of the cost function, this is precisely the quadratic (MSE) cost function. Maximizing the likelihood function is the same as minimizing the quadratic (MSE) cost function. This the *real* reason that the quadratic MSE cost function is so widely used in regression contexts.

## Gradient of the Linear Regression Cost Function
We've justified our cost function in terms of maximum likelihood estimation. Now, we compute the gradient of the cost function, with respect to parameters $\theta$, in order to determine the direction that decreases the cost function the most. Recall, the gradient, $\nabla J(\theta)$, is a vector-valued function, in which the vector contains the partial derivatives of the cost function with respect to each of the parameters $\theta$. We derive $\nabla J(\theta)$ as follows:

$$
\begin{equation}
\begin{split}
\frac{\partial}{\partial\theta_j}J(\theta)&=\frac{\partial}{\partial \theta_j}\bigg(\frac{1}{2m}\displaystyle\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2\bigg)\\
\frac{\partial}{\partial\theta_j}J(\theta)&=\frac{1}{2m}\displaystyle\sum_{i=1}^m\frac{\partial}{\partial \theta_j}(h_\theta(x^{(i)})-y^{(i)})^2\hspace{50mm}\text{derivative of sum is sum of derivative}\\
\frac{\partial}{\partial\theta_j}&=\frac{1}{2m}\displaystyle\sum_{i=1}^m2(h_\theta(x^{(i)})-y^{(i)})\frac{\partial}{\partial \theta_j}(h_\theta(x^{(i)})-y^{(i)})\hspace{22mm}\text{power+chain rule}\\
\frac{\partial}{\partial\theta_j}&=\frac{1}{2m}\displaystyle\sum_{i=1}^m2(h_\theta(x^{(i)})-y^{(i)})\frac{\partial}{\partial \theta_j}(\displaystyle\sum_{j=0}^n\theta_j^Tx_j^{(i)}-y^{(i)})\hspace{17mm}\text{substituting}\space \theta^Tx\\
\nabla J_\theta&=\frac{\partial}{\partial\theta_j}=\frac{1}{m}\displaystyle\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}\space(\text{for every j})\hspace{19mm}\text{power rule+algebraic simplification}
\end{split}
\end{equation}
$$
Again, this formulation assumes that we have included a bias term $x_0=1$ 


## Linear Regression with Gradient Descent Parameter Update Rule
Now that we have our gradient, we can iteratively apply the least mean squares (LMS) update rule.
$$
\theta_j := \theta_j-\alpha\nabla J_\theta
$$
The LMS update rule has several properties that seem natural. For instance, we update the parameters $\theta$ by *subtracting* the scaled gradient in order to move in the direction of steepest *descent*. In addition, the magnitude of the update is proportional to the **error** term $h_\theta(x^{(i)})-y^{(i)}$. If we encounter large errors in our training batch, the magnitude of the gradient is higher resulting in a larger change to the parameters $\theta$. On the other hand, if our predictions are close to the true values $y$, our error term is smaller, resulting in smaller changes to the parameters. Lastly, Gradient Descent can be susceptible to local minima; however, the error manifold for the linear regression cost function is convex and as such, has only a single global minimum.


## Linear Regression Model Summary
To summarize, the linear regression model representation is:
$$
\begin{equation}
\begin{split}
\text{Hypothesis Function}\hspace{30mm}&h_\theta(x)=\theta^Tx\\
\text{Cost Function}\hspace{30mm}&J(\theta)=\frac{1}{2m}\displaystyle\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2\\
\text{Gradient}\hspace{30mm}&\nabla{J\theta}=\frac{1}{m}\displaystyle\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}\space(\text{for every j})\\
\text{Parameter Update Rule}\hspace{30mm}&\theta_j := \theta_j-\alpha\nabla J_\theta
\end{split}
\end{equation}
$$

## Key Takeaways:  
1. The hypothesis space is defined as linear combinations of the parameters $\theta$ and the features $X$.   
2. We therefore assume that:     

* the true relationship between the independent and dependent variables is linear or approximately linear,    
* the effects of different independent variables on the expected value of the dependent variable are additive,   
* errors are independently and identically normally distributed with zero mean and constant variance

3. The cost function most commonly used for regression tasks is the quadratic MSE cost function. Its use is justified under maximum likelihood estimation inference framework.    
 


With that, let's move on to the Logistic Regression model representation.


